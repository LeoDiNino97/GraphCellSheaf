{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a toy-case topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a toy topology for our example\n",
    "\n",
    "nodes = [i for i in range(7)]\n",
    "edges = [\n",
    "    (0,1),\n",
    "    (0,3),\n",
    "    (0,6),\n",
    "    (1,2),\n",
    "    (1,5),\n",
    "    (2,4),\n",
    "    (4,6),\n",
    "    (5,6)\n",
    "]\n",
    "\n",
    "V = 7\n",
    "E = len(edges)\n",
    "\n",
    "d = 20                                           # Node and edges stalks dimension\n",
    "\n",
    "F = {\n",
    "    e:{\n",
    "        e[0]:np.random.randn(d,d),\n",
    "        e[1]:np.random.randn(d,d)\n",
    "        } \n",
    "        for e in edges\n",
    "    }                                           # Incidency linear maps\n",
    "\n",
    "# Sheaf representation \n",
    "\n",
    "# Coboundary map\n",
    "\n",
    "B = np.zeros((d*E, d*V))\n",
    "\n",
    "for i in range(len(edges)):\n",
    "    edge = edges[i]\n",
    "\n",
    "    u = edge[0] \n",
    "    v = edge[1] \n",
    "\n",
    "    B_u = F[edge][u]\n",
    "    B_v = F[edge][v]\n",
    "\n",
    "    B[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "    B[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "# Sheaf Laplacian\n",
    "\n",
    "L_f = B.T @ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a smooth signals dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(from Hansen J., \"Learning sheaf Laplacians from smooth signals\")* \n",
    "\n",
    "In order to retrieve a dataset of smoothsignals, first of all we sample random gaussians vectors on the nodes of the graph. Then we smooth them according to their expansion in terms of the eigenvectors of the sheaf Laplacian $L_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's firstly define a dataset of random gaussian vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.random.randn(V*d,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the Fourier-domain embedded in the Laplacian spectrum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider a Tikhonov inspired procedure where we firstly project our dataset over the space spanned by the eigenvectors of the sheaf laplacian: namely $U$ the matrix collecting this eigenvectors we have \n",
    "\\begin{equation}\n",
    "    \\hat{x} = U^T x\n",
    "\\end{equation}\n",
    "\n",
    "So that defining $h(\\lambda) = \\frac{1}{1 + 10\\lambda}$ and $H = \\mathrm{diag}\\{h(\\lambda)\\}_{\\lambda}$, we now have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = H(\\Lambda) \\hat{x}\n",
    "\\end{equation}\n",
    "\n",
    "and finally our dataset is just reprojected back into the vertex domain:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = U H(\\Lambda) \\hat{x} = U H(\\Lambda) U^T x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda, U = np.linalg.eig(L_f)\n",
    "H = 1/(1 + 10*Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = U @ np.diag(H) @ U.T @ X\n",
    "\n",
    "Y += np.random.normal(0, 10e-2, size=Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10690.130358964443"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(X.T @ L_f @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120.33265407669947"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(Y.T @ L_f @ Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def premultiplier(Xu, Xv):\n",
    "    uu = np.linalg.inv(Xu @ Xu.T)\n",
    "    uv = Xu @ Xv.T\n",
    "    vv = np.linalg.inv(Xv @ Xv.T)\n",
    "    vu = Xv @ Xu.T\n",
    "\n",
    "    return (uu, uv, vv, vu)\n",
    "\n",
    "def chi_u(uu, uv, vv, vu):\n",
    "\n",
    "    return ((uu @ uv - np.eye(uu.shape[0])) @ vv @ np.linalg.inv(vu @ uu @ uv @ vv - np.eye(uu.shape[0])) @ vu - np.eye(uu.shape[0])) @ uu\n",
    "\n",
    "def chi_v(uu, uv, vv, vu):\n",
    "\n",
    "    return (uu @ uv - np.eye(uu.shape[0])) @ vv @ np.linalg.inv(vu @ uu @ uv @ vv - np.eye(uu.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 0\n",
    "\n",
    "maps = {\n",
    "    edge : {\n",
    "        edge[0] : np.zeros((d,d)),\n",
    "        edge[1] : np.zeros((d,d))\n",
    "    }\n",
    "for edge in combinations(nodes, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:00, 2623.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for e in tqdm(combinations(nodes,2)):\n",
    "    u = e[0]\n",
    "    v = e[1]\n",
    "\n",
    "    X_u = Y[u*d:(u+1)*d,:]\n",
    "    X_v = Y[v*d:(v+1)*d,:]\n",
    "    uu, uv, vv, vu = premultiplier(X_u, X_v)\n",
    "\n",
    "    maps[e][u] = chi_u(uu, uv, vv, vu)\n",
    "    maps[e][v] = chi_u(uu, uv, vv, vu)\n",
    "    \n",
    "    T += np.trace(maps[e][u]) + np.trace(maps[e][u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_laplacian_metrics(\n",
    "        V:int,\n",
    "        edges:list,\n",
    "        d:int,\n",
    "        maps:dict,\n",
    "        X:np.array,\n",
    "        L_f:np.array,\n",
    "        ) -> dict:\n",
    "    \n",
    "    '''\n",
    "    Retrieve the first E edges based on their expressed energy and compute metrics of similarities with the original laplacian.\n",
    "\n",
    "    Parameters:\n",
    "    - V (int): number of nodes.\n",
    "    - edges (list): edges of the underlying graph\n",
    "    - d (int): stalks dimensions\n",
    "    - maps (dict) dictionary of restriction maps \n",
    "    - X (np.array): dataset of shape (V*d, N) of smooth signals\n",
    "    - L_f (np.array): groundthruth for the sheaf laplacian\n",
    "    \n",
    "    Returns:\n",
    "    - dict: dictionary containing three metrics as chosen by Hansen: average entrywise L2 and L1 reconstruction error, precision in recovering the graph underlying the sheaf\n",
    "    '''\n",
    "\n",
    "    E = len(edges)\n",
    "\n",
    "    all_edges = list(combinations(range(V), 2))\n",
    "\n",
    "    energies = {\n",
    "        e : 0\n",
    "        for e in all_edges\n",
    "        }\n",
    "    \n",
    "    for e in all_edges:\n",
    "        u = e[0]\n",
    "        v = e[1]\n",
    "\n",
    "        X_u = X[u*d:(u+1)*d,:]\n",
    "        X_v = X[v*d:(v+1)*d,:]\n",
    "\n",
    "\n",
    "        F_u = maps[e][u]\n",
    "        F_v = maps[e][u]\n",
    "\n",
    "        \n",
    "        energies[e] = np.linalg.norm(F_u @ X_u - F_v @ X_v)\n",
    "\n",
    "    retrieved = sorted(energies.items(), key=lambda x:x[1])[:E]\n",
    "\n",
    "    B_hat = np.zeros((d*E, d*V))\n",
    "\n",
    "    for i in range(E):\n",
    "        edge = retrieved[i][0]\n",
    "\n",
    "        u = edge[0] \n",
    "        v = edge[1] \n",
    "\n",
    "        B_u = maps[edge][u]\n",
    "        B_v = maps[edge][v]\n",
    "\n",
    "        B_hat[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "        B_hat[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "    L_f_hat = B_hat.T @ B_hat\n",
    "\n",
    "    return {\n",
    "        \"AvgEntryWiseED_L2\" : np.sqrt(np.sum((L_f - L_f_hat)**2)) / L_f.size,\n",
    "        \"AvgEntryWiseED_L1\" : np.sqrt(np.sum(np.abs(L_f - L_f_hat))) / L_f.size,\n",
    "        \"SparsityAccuracy\" : len(set(list(map(lambda x: x[0], retrieved))).intersection(set(edges))) / E\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'AvgEntryWiseED_L2': 0.08243484378858681, 'AvgEntryWiseED_L1': 0.042754948419883934, 'SparsityAccuracy': 0.25}\n",
      "2 {'AvgEntryWiseED_L2': 0.08243395792320231, 'AvgEntryWiseED_L1': 0.04275495786216986, 'SparsityAccuracy': 0.25}\n",
      "3 {'AvgEntryWiseED_L2': 0.08243248155811667, 'AvgEntryWiseED_L1': 0.0427549845376903, 'SparsityAccuracy': 0.25}\n",
      "4 {'AvgEntryWiseED_L2': 0.08243041480917125, 'AvgEntryWiseED_L1': 0.04275502285511217, 'SparsityAccuracy': 0.25}\n",
      "5 {'AvgEntryWiseED_L2': 0.08242775783856159, 'AvgEntryWiseED_L1': 0.04275507212031839, 'SparsityAccuracy': 0.25}\n",
      "6 {'AvgEntryWiseED_L2': 0.08242451085485455, 'AvgEntryWiseED_L1': 0.042755132333271134, 'SparsityAccuracy': 0.25}\n",
      "7 {'AvgEntryWiseED_L2': 0.08242067411301086, 'AvgEntryWiseED_L1': 0.04275520349392413, 'SparsityAccuracy': 0.25}\n",
      "8 {'AvgEntryWiseED_L2': 0.08241624791441247, 'AvgEntryWiseED_L1': 0.04275528560222273, 'SparsityAccuracy': 0.25}\n",
      "9 {'AvgEntryWiseED_L2': 0.08241123260689494, 'AvgEntryWiseED_L1': 0.04275537865810385, 'SparsityAccuracy': 0.25}\n",
      "10 {'AvgEntryWiseED_L2': 0.08240562858478477, 'AvgEntryWiseED_L1': 0.04275548266149603, 'SparsityAccuracy': 0.25}\n",
      "11 {'AvgEntryWiseED_L2': 0.0823994362889417, 'AvgEntryWiseED_L1': 0.04275559761231936, 'SparsityAccuracy': 0.25}\n",
      "12 {'AvgEntryWiseED_L2': 0.08239265620680611, 'AvgEntryWiseED_L1': 0.042755723510485545, 'SparsityAccuracy': 0.25}\n",
      "13 {'AvgEntryWiseED_L2': 0.08238528887245121, 'AvgEntryWiseED_L1': 0.042755860355897866, 'SparsityAccuracy': 0.25}\n",
      "14 {'AvgEntryWiseED_L2': 0.08237733486664042, 'AvgEntryWiseED_L1': 0.04275600814845124, 'SparsityAccuracy': 0.25}\n",
      "15 {'AvgEntryWiseED_L2': 0.08236879481688954, 'AvgEntryWiseED_L1': 0.04275616688803213, 'SparsityAccuracy': 0.25}\n",
      "16 {'AvgEntryWiseED_L2': 0.08235966939753406, 'AvgEntryWiseED_L1': 0.04275633657451861, 'SparsityAccuracy': 0.25}\n",
      "17 {'AvgEntryWiseED_L2': 0.08234995932980138, 'AvgEntryWiseED_L1': 0.04275651720778034, 'SparsityAccuracy': 0.25}\n",
      "18 {'AvgEntryWiseED_L2': 0.08233966538188806, 'AvgEntryWiseED_L1': 0.042756708787678586, 'SparsityAccuracy': 0.25}\n",
      "19 {'AvgEntryWiseED_L2': 0.08232878836904195, 'AvgEntryWiseED_L1': 0.042756911314066205, 'SparsityAccuracy': 0.25}\n",
      "20 {'AvgEntryWiseED_L2': 0.08231732915364953, 'AvgEntryWiseED_L1': 0.04275712478678764, 'SparsityAccuracy': 0.25}\n",
      "21 {'AvgEntryWiseED_L2': 0.08230528864532795, 'AvgEntryWiseED_L1': 0.042757349205678946, 'SparsityAccuracy': 0.25}\n",
      "22 {'AvgEntryWiseED_L2': 0.08229266780102235, 'AvgEntryWiseED_L1': 0.042757584570567754, 'SparsityAccuracy': 0.25}\n",
      "23 {'AvgEntryWiseED_L2': 0.08227946762510792, 'AvgEntryWiseED_L1': 0.04275783088127332, 'SparsityAccuracy': 0.25}\n",
      "24 {'AvgEntryWiseED_L2': 0.08226568916949718, 'AvgEntryWiseED_L1': 0.04275808813760647, 'SparsityAccuracy': 0.25}\n",
      "25 {'AvgEntryWiseED_L2': 0.08225133353375205, 'AvgEntryWiseED_L1': 0.04275835633936964, 'SparsityAccuracy': 0.25}\n",
      "26 {'AvgEntryWiseED_L2': 0.08223640186520106, 'AvgEntryWiseED_L1': 0.04275863548635687, 'SparsityAccuracy': 0.25}\n",
      "27 {'AvgEntryWiseED_L2': 0.08222089535906155, 'AvgEntryWiseED_L1': 0.0427589255783538, 'SparsityAccuracy': 0.25}\n",
      "28 {'AvgEntryWiseED_L2': 0.08220481525856672, 'AvgEntryWiseED_L1': 0.04275922661513766, 'SparsityAccuracy': 0.25}\n",
      "29 {'AvgEntryWiseED_L2': 0.08218816285509786, 'AvgEntryWiseED_L1': 0.042759538596477284, 'SparsityAccuracy': 0.25}\n",
      "30 {'AvgEntryWiseED_L2': 0.08217093948832146, 'AvgEntryWiseED_L1': 0.042759861522133125, 'SparsityAccuracy': 0.25}\n",
      "31 {'AvgEntryWiseED_L2': 0.08215314654633143, 'AvgEntryWiseED_L1': 0.04276019539185722, 'SparsityAccuracy': 0.25}\n",
      "32 {'AvgEntryWiseED_L2': 0.08213478546579615, 'AvgEntryWiseED_L1': 0.0427607920709838, 'SparsityAccuracy': 0.25}\n",
      "33 {'AvgEntryWiseED_L2': 0.08211585773211066, 'AvgEntryWiseED_L1': 0.04276143005525947, 'SparsityAccuracy': 0.25}\n",
      "34 {'AvgEntryWiseED_L2': 0.08209636487955384, 'AvgEntryWiseED_L1': 0.04276208765985838, 'SparsityAccuracy': 0.25}\n",
      "35 {'AvgEntryWiseED_L2': 0.08207630849145048, 'AvgEntryWiseED_L1': 0.042762764883875384, 'SparsityAccuracy': 0.25}\n",
      "36 {'AvgEntryWiseED_L2': 0.0820556902003385, 'AvgEntryWiseED_L1': 0.042763461726378384, 'SparsityAccuracy': 0.25}\n",
      "37 {'AvgEntryWiseED_L2': 0.08203451168814102, 'AvgEntryWiseED_L1': 0.042764178186408305, 'SparsityAccuracy': 0.25}\n",
      "38 {'AvgEntryWiseED_L2': 0.0820127746863436, 'AvgEntryWiseED_L1': 0.04276491426297919, 'SparsityAccuracy': 0.25}\n",
      "39 {'AvgEntryWiseED_L2': 0.0819904809761762, 'AvgEntryWiseED_L1': 0.042765669955078106, 'SparsityAccuracy': 0.25}\n",
      "40 {'AvgEntryWiseED_L2': 0.08196763238880049, 'AvgEntryWiseED_L1': 0.04276644526166522, 'SparsityAccuracy': 0.25}\n",
      "41 {'AvgEntryWiseED_L2': 0.08194423080550182, 'AvgEntryWiseED_L1': 0.0427672401816738, 'SparsityAccuracy': 0.25}\n",
      "42 {'AvgEntryWiseED_L2': 0.08192027815788643, 'AvgEntryWiseED_L1': 0.0427680547140102, 'SparsityAccuracy': 0.25}\n",
      "43 {'AvgEntryWiseED_L2': 0.08189577642808347, 'AvgEntryWiseED_L1': 0.04276888885755385, 'SparsityAccuracy': 0.25}\n",
      "44 {'AvgEntryWiseED_L2': 0.08187072764895215, 'AvgEntryWiseED_L1': 0.04276974261115734, 'SparsityAccuracy': 0.25}\n",
      "45 {'AvgEntryWiseED_L2': 0.08184513390429368, 'AvgEntryWiseED_L1': 0.042770615973646314, 'SparsityAccuracy': 0.25}\n",
      "46 {'AvgEntryWiseED_L2': 0.08181899732906843, 'AvgEntryWiseED_L1': 0.042771508943819606, 'SparsityAccuracy': 0.25}\n",
      "47 {'AvgEntryWiseED_L2': 0.08179232010961789, 'AvgEntryWiseED_L1': 0.04277242152044912, 'SparsityAccuracy': 0.25}\n",
      "48 {'AvgEntryWiseED_L2': 0.08176510448389154, 'AvgEntryWiseED_L1': 0.04277335370227997, 'SparsityAccuracy': 0.25}\n",
      "49 {'AvgEntryWiseED_L2': 0.08173735274167898, 'AvgEntryWiseED_L1': 0.04277430548803037, 'SparsityAccuracy': 0.25}\n",
      "50 {'AvgEntryWiseED_L2': 0.08170906722484664, 'AvgEntryWiseED_L1': 0.0427752768763917, 'SparsityAccuracy': 0.25}\n",
      "51 {'AvgEntryWiseED_L2': 0.0816802503275797, 'AvgEntryWiseED_L1': 0.04277626786602854, 'SparsityAccuracy': 0.25}\n",
      "52 {'AvgEntryWiseED_L2': 0.08165090449662885, 'AvgEntryWiseED_L1': 0.042777278455578605, 'SparsityAccuracy': 0.25}\n",
      "53 {'AvgEntryWiseED_L2': 0.08162103223156202, 'AvgEntryWiseED_L1': 0.042778308643652826, 'SparsityAccuracy': 0.25}\n",
      "54 {'AvgEntryWiseED_L2': 0.08159063608502094, 'AvgEntryWiseED_L1': 0.04277935842883531, 'SparsityAccuracy': 0.25}\n",
      "55 {'AvgEntryWiseED_L2': 0.08155971866298284, 'AvgEntryWiseED_L1': 0.042780427809683406, 'SparsityAccuracy': 0.25}\n",
      "56 {'AvgEntryWiseED_L2': 0.08152828262502672, 'AvgEntryWiseED_L1': 0.04278151678472763, 'SparsityAccuracy': 0.25}\n",
      "57 {'AvgEntryWiseED_L2': 0.08149633068460482, 'AvgEntryWiseED_L1': 0.04278262535247176, 'SparsityAccuracy': 0.25}\n",
      "58 {'AvgEntryWiseED_L2': 0.08146386560931886, 'AvgEntryWiseED_L1': 0.042783753511392804, 'SparsityAccuracy': 0.25}\n",
      "59 {'AvgEntryWiseED_L2': 0.081430890221201, 'AvgEntryWiseED_L1': 0.042784901259941, 'SparsityAccuracy': 0.25}\n",
      "60 {'AvgEntryWiseED_L2': 0.08139740739699995, 'AvgEntryWiseED_L1': 0.04278606859653987, 'SparsityAccuracy': 0.25}\n",
      "61 {'AvgEntryWiseED_L2': 0.08136342006847169, 'AvgEntryWiseED_L1': 0.04278725551958618, 'SparsityAccuracy': 0.25}\n",
      "62 {'AvgEntryWiseED_L2': 0.08132893122267507, 'AvgEntryWiseED_L1': 0.042788462027449986, 'SparsityAccuracy': 0.25}\n",
      "63 {'AvgEntryWiseED_L2': 0.08129394390227226, 'AvgEntryWiseED_L1': 0.042789688118474646, 'SparsityAccuracy': 0.25}\n",
      "64 {'AvgEntryWiseED_L2': 0.08125846120583395, 'AvgEntryWiseED_L1': 0.04279093379097679, 'SparsityAccuracy': 0.25}\n",
      "65 {'AvgEntryWiseED_L2': 0.0812224862881493, 'AvgEntryWiseED_L1': 0.042792199043246396, 'SparsityAccuracy': 0.25}\n",
      "66 {'AvgEntryWiseED_L2': 0.08118602236054075, 'AvgEntryWiseED_L1': 0.042793483873546735, 'SparsityAccuracy': 0.25}\n",
      "67 {'AvgEntryWiseED_L2': 0.08114907269118336, 'AvgEntryWiseED_L1': 0.04279478828011444, 'SparsityAccuracy': 0.25}\n",
      "68 {'AvgEntryWiseED_L2': 0.08111164060542903, 'AvgEntryWiseED_L1': 0.04279611226115948, 'SparsityAccuracy': 0.25}\n",
      "69 {'AvgEntryWiseED_L2': 0.08107372948613535, 'AvgEntryWiseED_L1': 0.04279745581486519, 'SparsityAccuracy': 0.25}\n",
      "70 {'AvgEntryWiseED_L2': 0.08103534277399904, 'AvgEntryWiseED_L1': 0.04279881893938827, 'SparsityAccuracy': 0.25}\n",
      "71 {'AvgEntryWiseED_L2': 0.08099648396789404, 'AvgEntryWiseED_L1': 0.042800201632858824, 'SparsityAccuracy': 0.25}\n",
      "72 {'AvgEntryWiseED_L2': 0.08095715662521433, 'AvgEntryWiseED_L1': 0.042801603893380336, 'SparsityAccuracy': 0.25}\n",
      "73 {'AvgEntryWiseED_L2': 0.08091736436222101, 'AvgEntryWiseED_L1': 0.04280302571902971, 'SparsityAccuracy': 0.25}\n",
      "74 {'AvgEntryWiseED_L2': 0.08087711085439431, 'AvgEntryWiseED_L1': 0.04280446710785729, 'SparsityAccuracy': 0.25}\n",
      "75 {'AvgEntryWiseED_L2': 0.08083639983678975, 'AvgEntryWiseED_L1': 0.042805928057886834, 'SparsityAccuracy': 0.25}\n",
      "76 {'AvgEntryWiseED_L2': 0.08079523510439889, 'AvgEntryWiseED_L1': 0.04280740856711555, 'SparsityAccuracy': 0.25}\n",
      "77 {'AvgEntryWiseED_L2': 0.08075362051251457, 'AvgEntryWiseED_L1': 0.04280890863351415, 'SparsityAccuracy': 0.25}\n",
      "78 {'AvgEntryWiseED_L2': 0.0807115599771005, 'AvgEntryWiseED_L1': 0.04281042825502677, 'SparsityAccuracy': 0.25}\n",
      "79 {'AvgEntryWiseED_L2': 0.0806690574751651, 'AvgEntryWiseED_L1': 0.04281196742957109, 'SparsityAccuracy': 0.25}\n",
      "80 {'AvgEntryWiseED_L2': 0.08062611704513963, 'AvgEntryWiseED_L1': 0.04281352615503828, 'SparsityAccuracy': 0.25}\n",
      "81 {'AvgEntryWiseED_L2': 0.08058274278726082, 'AvgEntryWiseED_L1': 0.042815104429293, 'SparsityAccuracy': 0.25}\n",
      "82 {'AvgEntryWiseED_L2': 0.08053893886395742, 'AvgEntryWiseED_L1': 0.042816702250173505, 'SparsityAccuracy': 0.25}\n",
      "83 {'AvgEntryWiseED_L2': 0.08049470950024099, 'AvgEntryWiseED_L1': 0.04281831961549157, 'SparsityAccuracy': 0.25}\n",
      "84 {'AvgEntryWiseED_L2': 0.08045005898410079, 'AvgEntryWiseED_L1': 0.04281995652303251, 'SparsityAccuracy': 0.25}\n",
      "85 {'AvgEntryWiseED_L2': 0.08040499166690279, 'AvgEntryWiseED_L1': 0.04282161297055529, 'SparsityAccuracy': 0.25}\n",
      "86 {'AvgEntryWiseED_L2': 0.08035951196379258, 'AvgEntryWiseED_L1': 0.04282328895579241, 'SparsityAccuracy': 0.25}\n",
      "87 {'AvgEntryWiseED_L2': 0.08031362435410212, 'AvgEntryWiseED_L1': 0.04282498447645001, 'SparsityAccuracy': 0.25}\n",
      "88 {'AvgEntryWiseED_L2': 0.08026733338176059, 'AvgEntryWiseED_L1': 0.042826699530207846, 'SparsityAccuracy': 0.25}\n",
      "89 {'AvgEntryWiseED_L2': 0.08022064365570877, 'AvgEntryWiseED_L1': 0.04282843411471933, 'SparsityAccuracy': 0.25}\n",
      "90 {'AvgEntryWiseED_L2': 0.08017355985031735, 'AvgEntryWiseED_L1': 0.04283018822761151, 'SparsityAccuracy': 0.25}\n",
      "91 {'AvgEntryWiseED_L2': 0.08012608670580872, 'AvgEntryWiseED_L1': 0.04283196186648517, 'SparsityAccuracy': 0.25}\n",
      "92 {'AvgEntryWiseED_L2': 0.08007822902868247, 'AvgEntryWiseED_L1': 0.0428337550289147, 'SparsityAccuracy': 0.25}\n",
      "93 {'AvgEntryWiseED_L2': 0.08002999169214434, 'AvgEntryWiseED_L1': 0.042835567712448265, 'SparsityAccuracy': 0.25}\n",
      "94 {'AvgEntryWiseED_L2': 0.07998137963653855, 'AvgEntryWiseED_L1': 0.04283739991460773, 'SparsityAccuracy': 0.25}\n",
      "95 {'AvgEntryWiseED_L2': 0.0799323978697835, 'AvgEntryWiseED_L1': 0.04283925163288868, 'SparsityAccuracy': 0.25}\n",
      "96 {'AvgEntryWiseED_L2': 0.07988305146781073, 'AvgEntryWiseED_L1': 0.04284112286476052, 'SparsityAccuracy': 0.25}\n",
      "97 {'AvgEntryWiseED_L2': 0.0798333455750069, 'AvgEntryWiseED_L1': 0.04284301360766639, 'SparsityAccuracy': 0.25}\n",
      "98 {'AvgEntryWiseED_L2': 0.0797832854046591, 'AvgEntryWiseED_L1': 0.0428449238590232, 'SparsityAccuracy': 0.25}\n",
      "99 {'AvgEntryWiseED_L2': 0.07973287623940274, 'AvgEntryWiseED_L1': 0.04284685361622173, 'SparsityAccuracy': 0.25}\n"
     ]
    }
   ],
   "source": [
    "for N in range(1,100):\n",
    "    maps_ = {\n",
    "        edge : {\n",
    "            edge[0] : N/T * maps[edge][edge[0]],\n",
    "            edge[1] : N/T * maps[edge][edge[1]]\n",
    "        }\n",
    "        for edge in combinations(nodes, 2)\n",
    "    }\n",
    "    print(N,reconstructed_laplacian_metrics(len(nodes), edges, d, maps_, Y, L_f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
