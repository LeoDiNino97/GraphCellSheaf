{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cellular sheaves on graphs \n",
    "## Learning sheaf laplacian through minimum total variation approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A sharing optimization approach to the problem through ADMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we want to solve is the following one: \n",
    "$$\\underset{L}{\\mathrm{min}} \\ \\ \\mathrm{tr}(X^TLX)$$\n",
    "\n",
    "that can be recasted as following:\n",
    "$$\\underset{\\{B_e^TB_e\\}_{e \\in \\mathcal{E}}}{\\mathrm{min}} \\ \\ \\mathrm{tr}(\\sum_e X^TB_e^TB_eX)$$\n",
    "\n",
    "Leveraging trace linearity and adding the regularization term we finally have:\n",
    "\n",
    "$$\\underset{\\{B_e^TB_e\\}_{e \\in \\mathcal{E}}}{\\mathrm{min}} \\ \\ \\sum_e \\mathrm{tr}(X^TB_e^TB_eX) - \\lambda \\log \\mathrm{det} (\\sum_e B_e^TB_e)$$\n",
    "\n",
    "This problem is a sharing optimization one where we are basically linking the local optimization over each possible edge to the global learning problem of the laplacian. We can leverage decomposition over the local optimization, while we have no decomposition on the regularization term. If we use the usual techniques of decoupling likelihood and regularization, we get the following problem: \n",
    "\\begin{cases}\n",
    "\\underset{{\\{B_e^T B_e\\}}_{e \\in \\mathcal{E}}}{\\mathrm{min}} \\sum_e tr(X^TB_e^TB_eX) - \\lambda \\log \\mathrm{det}(\\sum_e D_e^T D_e) \\\\\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ B_e^TB_e - D_e^TD_e = 0, \\ \\ \\ \\ \\ \\ \\ \\forall e \\in \\mathcal{E}\n",
    "\\end{cases}\n",
    "This problem can be solved in an ADMM fashion with respect to the quadratic terms: this will help us in recasting the step $(2)$ using the usual trick in dealing with shared optimization to decrease the number of variables. \n",
    "\\begin{gather}\n",
    "(B_e^TB_e)^{k+1} = \\underset{B_e^TB_e}{\\mathrm{argmin}} \\{ \\mathrm{tr}(X^TB_e^TB_eX) + \\frac{\\rho}{2}||B_e^TB_e - (D_e^TD_e)^k + u_e^k||^2\\} \\\\ \n",
    "\\{D_e^TD_e\\}^{k+1}_{e \\in \\mathcal{E}} = \\underset{\\{D_e^TD_e\\}_{e \\in \\mathcal{E}} }{\\mathrm{argmin}} \\{ -\\lambda \\log \\mathrm{det} (\\sum_e D_e^TD_e) + \\frac{\\rho}{2} ||(B_e^TB_e)^{k+1} - D_e^TD_e + u_e^k||^2\\} \\\\ \n",
    "u_e^{k+1} = u_e^k + [(B_e^TB_e)^{k+1} - (D_e^TD_e)^{k+1}]\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The global update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's focus for now on step $(2)$ that will yield a proximal mapping step if treaten properly. Setting \n",
    "$$ \\overline{L} = \\frac{1}{E}\\sum_e D_e^TD_e, \\ \\ \\ E = |\\mathcal{E}| $$\n",
    "$$ a_e = (B_e^TB_e)^{k+1} + u_e^k $$\n",
    "we can rewrite step $(2)$ as following:\n",
    "\n",
    "\\begin{cases}\n",
    "\\underset{\\{D_e^TD_e\\}_{e \\in \\mathcal{E}}}{\\mathrm{min}} \\ \\ \\ -\\lambda \\log \\mathrm{det} (E \\overline{L}) + \\frac{\\rho}{2} ||D_e^TD_e - a_e||^2 \\\\ \\nonumber\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\overline{L} - \\frac{1}{E}\\sum_e D_e^TD_e = 0\n",
    "\\end{cases}\n",
    "The lagrangian of the problem is \n",
    "\n",
    "$$ \\mathcal{L} = -\\lambda \\log \\mathrm{det} (E \\overline {L}) + \\frac{\\rho}{2} ||D_e^TD_e - a_e||^2 + \\nu (\\overline{L} - \\frac{1}{E}\\sum_e D_e^TD_e) $$\n",
    "So that\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial D_e^TD_e} = \\rho(D_e^TD_e - a_e) - \\frac{\\nu}{E} = 0 $$\n",
    "\n",
    "implying the following\n",
    "\n",
    "$$ D_e^TD_e = a_e + \\frac{\\nu}{\\rho E}$$\n",
    "Now we can rewrite $\\overline{L}$ as \n",
    "$$ \\overline{L} = \\frac{1}{E}\\sum_e D_e^TD_e = \\frac{1}{E}\\sum_e (a_e + \\frac{\\nu}{\\rho E}) = \\overline{a} + \\frac{\\nu}{\\rho E} $$\n",
    "\n",
    "so that \n",
    "\n",
    "$$ \\frac{\\nu}{\\rho E} = \\overline{L} - \\overline{a} $$\n",
    "Finally we have the following $(4)$: \n",
    "\n",
    "$$ (D_e^TD_e) ^{k+1} = a_e + \\overline{L} - \\overline{a} = (B_e^TB_e)^{k+1} + u_e^k + \\overline{L} - (\\overline{B_e^TB_e})^{k+1} - \\overline{u}^k $$\n",
    "\n",
    "This means that the multipliers are shared in the following way: \n",
    "\n",
    "$$ u_e^{k+1} = \\bcancel{u_e ^ k} + \\bcancel{(B_e^TB_e)^{k+1}} - \\bcancel{(B_e^TB_e)^{k+1}}  - \\bcancel{u_e^k} - \\overline{L} + (\\overline{B_e^TB_e})^{k+1} + \\overline{u}^k $$\n",
    "$$ u^{k+1} = u^k + [(\\overline{B_e^TB_e})^{k+1} - \\overline{L}^{k+1}] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging $(4)$ into $(1)$ we get the following update equation: \n",
    "\n",
    "$$ (B_e^TB_e)^{k+1} = \\underset{B_e^TB_e}{\\mathrm{argmin}} \\ \\ \\ \\  \\{ \\mathrm{tr}(X^TB_e^TB_eX) + \\frac{\\rho}{2}||B_e^TB_e - (B_e^TB_e)^{k} - \\overline{L}^k + (\\overline{B_e^TB_e})^{k} + \\overline{u}^k||^2\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should highlight that plugging $(4)$ into $(2)$ we get\n",
    "\n",
    "$$ \\overline{L}^{k+1} = \\underset{\\overline{L}}{\\mathrm{argmin}} \\{ -\\lambda \\log \\mathrm{det} (E\\overline{L}) + \\frac{\\rho E}{2} || \\overline{L} - (\\overline{B_e^TB_e}^{k+1} + u^k) || ^ 2\\} $$\n",
    "\n",
    "which is nothing but|\n",
    "$$ \\overline{L}^{k+1} = \\mathrm{prox}_{-\\frac{\\lambda}{\\rho E} \\log \\mathrm{det}} (\\overline{B_e^TB_e}^{k+1} + u^k) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage the results of *H. H. Bauschke and P. L. Combettes: Convex Analysis and Monotone Operator Theory in Hilbert Spaces (2nd Edition). Springer, New York, 2017* in deriving the proximal mapping of the logdet of a symmetric positive semidefinite matrix, which $\\overline{L}$ is by design. In particular, given $X \\in \\mathbb{R}^{n \\times n}$ and its spectral decomposition $X = U^T \\mathrm{Diag}(s) U$, the proximal mapping of $-\\log\\det(X)$ is\n",
    " \n",
    "\\begin{equation}\n",
    "\\mathrm{prox}_{-\\gamma\\log\\det} (X)= U \\ \\mathrm{Diag}(z) \\ U^T, \\ z = \\frac{1}{2}(s + \\sqrt{s^2 + 4\\gamma})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The local updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now we have collected the following ADMM scheme: \n",
    "\n",
    "\\begin{gather}\n",
    "(B_e^TB_e)^{k+1} = \\underset{B_e^TB_e}{\\mathrm{argmin}} \\ \\ \\ \\  \\{ \\mathrm{tr}(X^TB_e^TB_eX) + \\frac{\\rho}{2}||B_e^TB_e - (B_e^TB_e)^{k} - \\overline{L}^k + (\\overline{B_e^TB_e})^{k} + u^k||^2\\} \\\\ \n",
    "\\overline{L}^{k+1} = \\mathrm{prox}_{-\\frac{\\lambda}{\\rho E} \\log \\mathrm{det}} (\\overline{B_e^TB_e}^{k+1} + u^k) \\\\\n",
    "u^{k+1} = u^k + [(\\overline{B_e^TB_e})^{k+1} - \\overline{L}^{k+1}]\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on step $(1)$, trying to define a proper learning procedure. First of all, notice that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{tr}(X^TB_e^TB_eX) = ||B_eX||^2 = ||\\mathcal{F}_{u \\triangleleft e}X_u - \\mathcal{F}_{v \\triangleleft e}X_v||^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we can also reason similarly to the second term in the function, proposing the following decomposition: \n",
    "\n",
    "$$ \\frac{\\rho}{2}||B_e^TB_e - (B_e^TB_e)^{k} - \\overline{L}^k + (\\overline{B_e^TB_e})^{k} + \\overline{u}^k||^2 = \\frac{\\rho}{2} [l_{(u,u)}^2 + l_{(u,v)}^2 + l_{(v,u)}^2 + l_{(v,v)}^2] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "\n",
    "\\begin{gather}\n",
    "l_{(u,u)}^2 = || \\mathcal{F}_{u \\triangleleft e}^T \\mathcal{F}_{u \\triangleleft e} - (\\mathcal{F}_{u \\triangleleft e}^T \\mathcal{F}_{u \\triangleleft e})^k - \\overline{L}^k_{(u,u)} + (\\overline{B_e^TB_e})^{k}_{(u,u)} + u^k_{(u,u)} || ^2 \\\\ \\nonumber\n",
    "l_{(u,v)}^2 = || - \\mathcal{F}_{u \\triangleleft e}^T \\mathcal{F}_{v \\triangleleft e} + (\\mathcal{F}_{u \\triangleleft e}^T \\mathcal{F}_{v \\triangleleft e})^k - \\overline{L}^k_{(u,v)} + (\\overline{B_e^TB_e})^{k}_{(u,v)} + u^k_{(u,v)} || ^2 \\\\ \\nonumber\n",
    "l_{(v,u)}^2 = || - \\mathcal{F}_{v \\triangleleft e}^T \\mathcal{F}_{u \\triangleleft e} + (\\mathcal{F}_{u \\triangleleft e}^T \\mathcal{F}_{v \\triangleleft e})^k - \\overline{L}^k_{(v,u)} + (\\overline{B_e^TB_e})^{k}_{(v,u)} + u^k_{(v,u)} || ^2 \\\\ \\nonumber\n",
    "l_{(v,v)}^2 = || \\mathcal{F}_{v \\triangleleft e}^T \\mathcal{F}_{v \\triangleleft e} - (\\mathcal{F}_{v \\triangleleft e}^T \\mathcal{F}_{v \\triangleleft e})^k - \\overline{L}^k_{(v,v)} + (\\overline{B_e^TB_e})^{k}_{(v,v)} + u^k_{(v,v)} || ^2 \\\\ \\nonumber\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recast the local update as a convezx problem in the restriction maps of edge $e$: \n",
    "\n",
    "$$ \\underset{\\mathcal{F}_{u \\triangleleft e}, \\mathcal{F}_{v \\triangleleft e}}{\\mathrm{min}} \\frac{1}{2} ||\\mathcal{F}_{u \\triangleleft e}X_u - \\mathcal{F}_{v \\triangleleft e}X_v||^2 + \\frac{\\rho}{2} [l_{(u,u)}^2 + l_{(u,v)}^2 + l_{(v,u)}^2 + l_{(v,v)}^2] = \\underset{\\mathcal{F}_{u \\triangleleft e}, \\mathcal{F}_{v \\triangleleft e}}{\\mathrm{min}} G(\\mathcal{F}_{u \\triangleleft e}, \\mathcal{F}_{v \\triangleleft e})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tackle it through a gradient based numeric approximation:\n",
    "$$ \\frac{\\partial}{\\partial \\mathcal{F}_{u \\triangleleft e}} G(\\mathcal{F}_{u \\triangleleft e}, \\mathcal{F}_{v \\triangleleft e}) = (\\mathcal{F}_{u \\triangleleft e}X_u - \\mathcal{F}_{v \\triangleleft e}X_v)X_u^T + \\rho[\\mathcal{F}_{u \\triangleleft e}l_{(u,u)} - 2\\mathcal{F}_{v \\triangleleft e}l_{(v,u)}] $$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathcal{F}_{v \\triangleleft e}} G(\\mathcal{F}_{u \\triangleleft e}, \\mathcal{F}_{v \\triangleleft e}) = -(\\mathcal{F}_{u \\triangleleft e}X_u - \\mathcal{F}_{v \\triangleleft e}X_v)X_v^T + \\rho[- 2\\mathcal{F}_{u \\triangleleft e}l_{(u,v)} + \\mathcal{F}_{v \\triangleleft e}l_{(v,v)} ] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a toy-case topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a toy topology for our example\n",
    "\n",
    "nodes = [i for i in range(7)]\n",
    "edges = [\n",
    "    (0,1),\n",
    "    (0,3),\n",
    "    (0,6),\n",
    "    (1,2),\n",
    "    (1,5),\n",
    "    (2,4),\n",
    "    (4,6),\n",
    "    (5,6)\n",
    "]\n",
    "\n",
    "V = 7\n",
    "E = len(edges)\n",
    "\n",
    "d = 3                                           # Node and edges stalks dimension\n",
    "\n",
    "F = {\n",
    "    e:{\n",
    "        e[0]:np.random.randn(d,d),\n",
    "        e[1]:np.random.randn(d,d)\n",
    "        } \n",
    "        for e in edges\n",
    "    }                                           # Incidency linear maps\n",
    "\n",
    "# Sheaf representation \n",
    "\n",
    "# Coboundary map\n",
    "\n",
    "B = np.zeros((d*E, d*V))\n",
    "\n",
    "for i in range(len(edges)):\n",
    "    edge = edges[i]\n",
    "\n",
    "    u = edge[0] \n",
    "    v = edge[1] \n",
    "\n",
    "    B_u = F[edge][u]\n",
    "    B_v = F[edge][v]\n",
    "\n",
    "    B[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "    B[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "# Sheaf Laplacian\n",
    "\n",
    "L_f = B.T @ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a smooth signals dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(from Hansen J., \"Learning sheaf Laplacians from smooth signals\")* \n",
    "\n",
    "In order to retrieve a dataset of smoothsignals, first of all we sample random gaussians vectors on the nodes of the graph. Then we smooth them according to their expansion in terms of the eigenvectors of the sheaf Laplacian $L_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's firstly define a dataset of random gaussian vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.random.randn(V*d,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the Fourier-domain embedded in the Laplacian spectrum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider a Tikhonov inspired procedure where we firstly project our dataset over the space spanned by the eigenvectors of the sheaf laplacian: namely $U$ the matrix collecting this eigenvectors we have \n",
    "\\begin{equation}\n",
    "    \\hat{x} = U^T x\n",
    "\\end{equation}\n",
    "\n",
    "So that defining $h(\\lambda) = \\frac{1}{1 + 10\\lambda}$ and $H = \\mathrm{diag}\\{h(\\lambda)\\}_{\\lambda}$, we now have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = H(\\Lambda) \\hat{x}\n",
    "\\end{equation}\n",
    "\n",
    "and finally our dataset is just reprojected back into the vertex domain:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = U H(\\Lambda) \\hat{x} = U H(\\Lambda) U^T x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda, U = np.linalg.eig(L_f)\n",
    "H = 1/(1 + 10*Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = U @ np.diag(H) @ U.T @ X\n",
    "\n",
    "Y += np.random.normal(0, 10e-2, size=Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16719.807896960592"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(X.T @ L_f @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153.59377846056066"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(Y.T @ L_f @ Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our centralized procedure over our toy case topology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [14:15<00:00,  8.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from controller import learning\n",
    "from itertools import combinations\n",
    "\n",
    "restriction_maps = learning(7, d, Y, 3e-4, 0.005, 5, 100, 100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now retrieve the energy expressed by each edge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges = list(combinations(range(V), 2))\n",
    "\n",
    "energies = {\n",
    "    e : 0\n",
    "    for e in all_edges\n",
    "    }\n",
    "\n",
    "for e in (all_edges):\n",
    "    u = e[0]\n",
    "    v = e[1]\n",
    "\n",
    "    X_u = Y[u*d:(u+1)*d,:]\n",
    "    X_v = Y[v*d:(v+1)*d,:]\n",
    "\n",
    "    F_u = restriction_maps[e][u]\n",
    "    F_v = restriction_maps[e][v]\n",
    "\n",
    "    energies[e] = np.linalg.norm(F_u @ X_u - F_v @ X_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider the first $E_0$ edges sorted accordingly to their energy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = sorted(energies.items(), key=lambda x:x[1])[:E]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reconstruct the sheaf laplacian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_hat = np.zeros((d*E, d*V))\n",
    "\n",
    "for i in range(E):\n",
    "    edge = retrieved[i][0]\n",
    "\n",
    "    u = edge[0] \n",
    "    v = edge[1] \n",
    "\n",
    "    B_u = restriction_maps[edge][u]\n",
    "    B_v = restriction_maps[edge][v]\n",
    "\n",
    "    B_hat[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "    B_hat[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "L_f_hat = B_hat.T @ B_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3455049318407991"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The metric chosen by Hansen for the evaluation was the average entry-wise euclidean distance\n",
    "\n",
    "np.sqrt(np.sum((L_f - L_f_hat)**2)) / L_f.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the precision of our procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(list(map(lambda x: x[0], retrieved))).intersection(set(edges))) / E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (0, 3), (0, 6), (1, 2), (1, 5), (2, 4), (4, 6), (5, 6)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 5), 14.297216996806172),\n",
       " ((5, 6), 14.332253159609547),\n",
       " ((3, 6), 14.951780329119764),\n",
       " ((1, 5), 15.620809773446537),\n",
       " ((1, 3), 17.754223826054606),\n",
       " ((1, 6), 19.470499728809315),\n",
       " ((0, 6), 20.12303756035205),\n",
       " ((3, 4), 20.83771648490231)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
