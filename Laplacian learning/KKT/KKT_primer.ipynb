{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a toy-case topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a toy topology for our example\n",
    "\n",
    "nodes = [i for i in range(7)]\n",
    "edges = [\n",
    "    (0,1),\n",
    "    (0,3),\n",
    "    (0,6),\n",
    "    (1,2),\n",
    "    (1,5),\n",
    "    (2,4),\n",
    "    (4,6),\n",
    "    (5,6)\n",
    "]\n",
    "\n",
    "V = 7\n",
    "E = len(edges)\n",
    "\n",
    "d = 3                                           # Node and edges stalks dimension\n",
    "\n",
    "F = {\n",
    "    e:{\n",
    "        e[0]:np.random.randn(d,d),\n",
    "        e[1]:np.random.randn(d,d)\n",
    "        } \n",
    "        for e in edges\n",
    "    }                                           # Incidency linear maps\n",
    "\n",
    "# Sheaf representation \n",
    "\n",
    "# Coboundary map\n",
    "\n",
    "B = np.zeros((d*E, d*V))\n",
    "\n",
    "for i in range(len(edges)):\n",
    "    edge = edges[i]\n",
    "\n",
    "    u = edge[0] \n",
    "    v = edge[1] \n",
    "\n",
    "    B_u = F[edge][u]\n",
    "    B_v = F[edge][v]\n",
    "\n",
    "    B[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "    B[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "# Sheaf Laplacian\n",
    "\n",
    "L_f = B.T @ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a smooth signals dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(from Hansen J., \"Learning sheaf Laplacians from smooth signals\")* \n",
    "\n",
    "In order to retrieve a dataset of smoothsignals, first of all we sample random gaussians vectors on the nodes of the graph. Then we smooth them according to their expansion in terms of the eigenvectors of the sheaf Laplacian $L_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's firstly define a dataset of random gaussian vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.random.randn(V*d,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the Fourier-domain embedded in the Laplacian spectrum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider a Tikhonov inspired procedure where we firstly project our dataset over the space spanned by the eigenvectors of the sheaf laplacian: namely $U$ the matrix collecting this eigenvectors we have \n",
    "\\begin{equation}\n",
    "    \\hat{x} = U^T x\n",
    "\\end{equation}\n",
    "\n",
    "So that defining $h(\\lambda) = \\frac{1}{1 + 10\\lambda}$ and $H = \\mathrm{diag}\\{h(\\lambda)\\}_{\\lambda}$, we now have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = H(\\Lambda) \\hat{x}\n",
    "\\end{equation}\n",
    "\n",
    "and finally our dataset is just reprojected back into the vertex domain:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = U H(\\Lambda) \\hat{x} = U H(\\Lambda) U^T x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda, U = np.linalg.eig(L_f)\n",
    "H = 1/(1 + 10*Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = U @ np.diag(H) @ U.T @ X\n",
    "\n",
    "Y += np.random.normal(0, 10e-2, size=Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15481.183983912217"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(X.T @ L_f @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175.8384078504842"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(Y.T @ L_f @ Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def premultiplier(Xu, Xv):\n",
    "    uu = np.linalg.inv(Xu @ Xu.T)\n",
    "    uv = Xu @ Xv.T\n",
    "    vv = np.linalg.inv(Xv @ Xv.T)\n",
    "    vu = Xv @ Xu.T\n",
    "\n",
    "    return (uu, uv, vv, vu)\n",
    "\n",
    "def chi_u(uu, uv, vv, vu):\n",
    "\n",
    "    return ((uu @ uv - np.eye(uu.shape[0])) @ vv @ np.linalg.inv(vu @ uu @ uv @ vv - np.eye(uu.shape[0])) @ vu - np.eye(uu.shape[0])) @ uu\n",
    "\n",
    "def chi_v(uu, uv, vv, vu):\n",
    "\n",
    "    return (uu @ uv - np.eye(uu.shape[0])) @ vv @ np.linalg.inv(vu @ uu @ uv @ vv - np.eye(uu.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 0\n",
    "\n",
    "maps = {\n",
    "    edge : {\n",
    "        edge[0] : np.zeros((d_e,d)),\n",
    "        edge[1] : np.zeros((d_e,d))\n",
    "    }\n",
    "for edge in combinations(nodes, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:00, 2624.49it/s]\n"
     ]
    }
   ],
   "source": [
    "for e in tqdm(combinations(nodes,2)):\n",
    "    u = e[0]\n",
    "    v = e[1]\n",
    "\n",
    "    X_u = Y[u*d:(u+1)*d,:]\n",
    "    X_v = Y[v*d:(v+1)*d,:]\n",
    "    uu, uv, vv, vu = premultiplier(X_u, X_v)\n",
    "\n",
    "    maps[e][u] = chi_u(uu, uv, vv, vu)\n",
    "    maps[e][v] = chi_u(uu, uv, vv, vu)\n",
    "    \n",
    "    T += np.trace(maps[e][u]) + np.trace(maps[e][v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Leonardo\\Documents\\GitHub\\GraphCellSheaf\\LaplacianLearning\\SharingOptimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'AvgEntryWiseED_L2': 0.11488197608032148, 'AvgEntryWiseED_L1': 0.05006601554304098, 'SparsityAccuracy': 0.5}\n",
      "2 {'AvgEntryWiseED_L2': 0.11487724807514738, 'AvgEntryWiseED_L1': 0.050065773841368004, 'SparsityAccuracy': 0.5}\n",
      "3 {'AvgEntryWiseED_L2': 0.11486936909521005, 'AvgEntryWiseED_L1': 0.05006537100265332, 'SparsityAccuracy': 0.5}\n",
      "4 {'AvgEntryWiseED_L2': 0.1148583406839833, 'AvgEntryWiseED_L1': 0.05006480702300726, 'SparsityAccuracy': 0.5}\n",
      "5 {'AvgEntryWiseED_L2': 0.11484416500321977, 'AvgEntryWiseED_L1': 0.05006408189698398, 'SparsityAccuracy': 0.5}\n",
      "6 {'AvgEntryWiseED_L2': 0.11482684483384031, 'AvgEntryWiseED_L1': 0.050063195617581234, 'SparsityAccuracy': 0.5}\n",
      "7 {'AvgEntryWiseED_L2': 0.11480638357707836, 'AvgEntryWiseED_L1': 0.05006214817624006, 'SparsityAccuracy': 0.5}\n",
      "8 {'AvgEntryWiseED_L2': 0.11478278525587911, 'AvgEntryWiseED_L1': 0.05006093956284431, 'SparsityAccuracy': 0.5}\n",
      "9 {'AvgEntryWiseED_L2': 0.114756054516554, 'AvgEntryWiseED_L1': 0.05005956976572024, 'SparsityAccuracy': 0.5}\n",
      "10 {'AvgEntryWiseED_L2': 0.1147261966306914, 'AvgEntryWiseED_L1': 0.050058038771635845, 'SparsityAccuracy': 0.5}\n",
      "11 {'AvgEntryWiseED_L2': 0.11469321749732346, 'AvgEntryWiseED_L1': 0.050056346565800304, 'SparsityAccuracy': 0.5}\n",
      "12 {'AvgEntryWiseED_L2': 0.11465712364535041, 'AvgEntryWiseED_L1': 0.05005449313186322, 'SparsityAccuracy': 0.5}\n",
      "13 {'AvgEntryWiseED_L2': 0.11461792223622264, 'AvgEntryWiseED_L1': 0.05005247845191387, 'SparsityAccuracy': 0.5}\n",
      "14 {'AvgEntryWiseED_L2': 0.11457562106688135, 'AvgEntryWiseED_L1': 0.050050302506480286, 'SparsityAccuracy': 0.5}\n",
      "15 {'AvgEntryWiseED_L2': 0.11453022857295879, 'AvgEntryWiseED_L1': 0.05004796527452835, 'SparsityAccuracy': 0.5}\n",
      "16 {'AvgEntryWiseED_L2': 0.1144817538322391, 'AvgEntryWiseED_L1': 0.05004546673346078, 'SparsityAccuracy': 0.5}\n",
      "17 {'AvgEntryWiseED_L2': 0.11443020656838032, 'AvgEntryWiseED_L1': 0.05004280685911601, 'SparsityAccuracy': 0.5}\n",
      "18 {'AvgEntryWiseED_L2': 0.11437559715489919, 'AvgEntryWiseED_L1': 0.050039985625767044, 'SparsityAccuracy': 0.5}\n",
      "19 {'AvgEntryWiseED_L2': 0.11431793661941916, 'AvgEntryWiseED_L1': 0.050037003006120184, 'SparsityAccuracy': 0.5}\n",
      "20 {'AvgEntryWiseED_L2': 0.11425723664818364, 'AvgEntryWiseED_L1': 0.05003385897131375, 'SparsityAccuracy': 0.5}\n",
      "21 {'AvgEntryWiseED_L2': 0.11419350959083466, 'AvgEntryWiseED_L1': 0.05003055349091665, 'SparsityAccuracy': 0.5}\n",
      "22 {'AvgEntryWiseED_L2': 0.11412676846545919, 'AvgEntryWiseED_L1': 0.05002708653292688, 'SparsityAccuracy': 0.5}\n",
      "23 {'AvgEntryWiseED_L2': 0.11405702696390364, 'AvgEntryWiseED_L1': 0.05002345806377006, 'SparsityAccuracy': 0.5}\n",
      "24 {'AvgEntryWiseED_L2': 0.11398429945735827, 'AvgEntryWiseED_L1': 0.0500196680482977, 'SparsityAccuracy': 0.5}\n",
      "25 {'AvgEntryWiseED_L2': 0.11390860100221283, 'AvgEntryWiseED_L1': 0.05001571644978557, 'SparsityAccuracy': 0.5}\n",
      "26 {'AvgEntryWiseED_L2': 0.11382994734618465, 'AvgEntryWiseED_L1': 0.05001160322993188, 'SparsityAccuracy': 0.5}\n",
      "27 {'AvgEntryWiseED_L2': 0.11374835493472064, 'AvgEntryWiseED_L1': 0.05000737081092891, 'SparsityAccuracy': 0.5}\n",
      "28 {'AvgEntryWiseED_L2': 0.11366384091767491, 'AvgEntryWiseED_L1': 0.050003026434562114, 'SparsityAccuracy': 0.5}\n",
      "29 {'AvgEntryWiseED_L2': 0.11357642315626268, 'AvgEntryWiseED_L1': 0.04999852368255005, 'SparsityAccuracy': 0.5}\n",
      "30 {'AvgEntryWiseED_L2': 0.11348612023029286, 'AvgEntryWiseED_L1': 0.04999386251209988, 'SparsityAccuracy': 0.5}\n",
      "31 {'AvgEntryWiseED_L2': 0.11339295144568004, 'AvgEntryWiseED_L1': 0.049989042878897, 'SparsityAccuracy': 0.5}\n",
      "32 {'AvgEntryWiseED_L2': 0.1132969368422374, 'AvgEntryWiseED_L1': 0.04998406473710284, 'SparsityAccuracy': 0.5}\n",
      "33 {'AvgEntryWiseED_L2': 0.11319809720175206, 'AvgEntryWiseED_L1': 0.04997892803935275, 'SparsityAccuracy': 0.5}\n",
      "34 {'AvgEntryWiseED_L2': 0.11309645405634414, 'AvgEntryWiseED_L1': 0.0499736327367537, 'SparsityAccuracy': 0.5}\n",
      "35 {'AvgEntryWiseED_L2': 0.1129920296971108, 'AvgEntryWiseED_L1': 0.04996817877888196, 'SparsityAccuracy': 0.5}\n",
      "36 {'AvgEntryWiseED_L2': 0.11288484718305639, 'AvgEntryWiseED_L1': 0.04996256611378069, 'SparsityAccuracy': 0.5}\n",
      "37 {'AvgEntryWiseED_L2': 0.1127749303503101, 'AvgEntryWiseED_L1': 0.04995679468795743, 'SparsityAccuracy': 0.5}\n",
      "38 {'AvgEntryWiseED_L2': 0.11266230382163205, 'AvgEntryWiseED_L1': 0.049950864446381554, 'SparsityAccuracy': 0.5}\n",
      "39 {'AvgEntryWiseED_L2': 0.11254699301620873, 'AvgEntryWiseED_L1': 0.049944775332481636, 'SparsityAccuracy': 0.5}\n",
      "40 {'AvgEntryWiseED_L2': 0.1124290241597389, 'AvgEntryWiseED_L1': 0.04993852728814273, 'SparsityAccuracy': 0.5}\n",
      "41 {'AvgEntryWiseED_L2': 0.11230842429481071, 'AvgEntryWiseED_L1': 0.04993212025370356, 'SparsityAccuracy': 0.5}\n",
      "42 {'AvgEntryWiseED_L2': 0.1121852212915702, 'AvgEntryWiseED_L1': 0.04992555416795368, 'SparsityAccuracy': 0.5}\n",
      "43 {'AvgEntryWiseED_L2': 0.11205944385868251, 'AvgEntryWiseED_L1': 0.049918828968130574, 'SparsityAccuracy': 0.5}\n",
      "44 {'AvgEntryWiseED_L2': 0.11193112155458537, 'AvgEntryWiseED_L1': 0.04991209625105994, 'SparsityAccuracy': 0.5}\n",
      "45 {'AvgEntryWiseED_L2': 0.11180028479903538, 'AvgEntryWiseED_L1': 0.04990579879811723, 'SparsityAccuracy': 0.5}\n",
      "46 {'AvgEntryWiseED_L2': 0.11166696488494701, 'AvgEntryWiseED_L1': 0.04989935900762983, 'SparsityAccuracy': 0.5}\n",
      "47 {'AvgEntryWiseED_L2': 0.11153119399052384, 'AvgEntryWiseED_L1': 0.049892776824482094, 'SparsityAccuracy': 0.5}\n",
      "48 {'AvgEntryWiseED_L2': 0.11139300519168163, 'AvgEntryWiseED_L1': 0.0498860521923103, 'SparsityAccuracy': 0.5}\n",
      "49 {'AvgEntryWiseED_L2': 0.11125243247476271, 'AvgEntryWiseED_L1': 0.0498791850535002, 'SparsityAccuracy': 0.5}\n",
      "50 {'AvgEntryWiseED_L2': 0.11110951074953977, 'AvgEntryWiseED_L1': 0.049872750530084727, 'SparsityAccuracy': 0.5}\n",
      "51 {'AvgEntryWiseED_L2': 0.1109642758625088, 'AvgEntryWiseED_L1': 0.04986644340994597, 'SparsityAccuracy': 0.5}\n",
      "52 {'AvgEntryWiseED_L2': 0.11081676461046847, 'AvgEntryWiseED_L1': 0.04986001057465577, 'SparsityAccuracy': 0.5}\n",
      "53 {'AvgEntryWiseED_L2': 0.11066701475438427, 'AvgEntryWiseED_L1': 0.0498534519755492, 'SparsityAccuracy': 0.5}\n",
      "54 {'AvgEntryWiseED_L2': 0.1105150650335351, 'AvgEntryWiseED_L1': 0.04984676756298406, 'SparsityAccuracy': 0.5}\n",
      "55 {'AvgEntryWiseED_L2': 0.11036095517993885, 'AvgEntryWiseED_L1': 0.049839957286338964, 'SparsityAccuracy': 0.5}\n",
      "56 {'AvgEntryWiseED_L2': 0.11020472593305417, 'AvgEntryWiseED_L1': 0.04983302109401145, 'SparsityAccuracy': 0.5}\n",
      "57 {'AvgEntryWiseED_L2': 0.11004641905475405, 'AvgEntryWiseED_L1': 0.049825958933415924, 'SparsityAccuracy': 0.5}\n",
      "58 {'AvgEntryWiseED_L2': 0.10988607734456744, 'AvgEntryWiseED_L1': 0.049818770750981754, 'SparsityAccuracy': 0.5}\n",
      "59 {'AvgEntryWiseED_L2': 0.10972374465518325, 'AvgEntryWiseED_L1': 0.0498115595970859, 'SparsityAccuracy': 0.5}\n",
      "60 {'AvgEntryWiseED_L2': 0.10955946590821185, 'AvgEntryWiseED_L1': 0.049804319512990435, 'SparsityAccuracy': 0.5}\n",
      "61 {'AvgEntryWiseED_L2': 0.10939328711019745, 'AvgEntryWiseED_L1': 0.049798600660613826, 'SparsityAccuracy': 0.5}\n",
      "62 {'AvgEntryWiseED_L2': 0.10922525536887483, 'AvgEntryWiseED_L1': 0.04979301637943155, 'SparsityAccuracy': 0.5}\n",
      "63 {'AvgEntryWiseED_L2': 0.10905541890966261, 'AvgEntryWiseED_L1': 0.04978734065522177, 'SparsityAccuracy': 0.5}\n",
      "64 {'AvgEntryWiseED_L2': 0.10888382709238506, 'AvgEntryWiseED_L1': 0.049781573456707515, 'SparsityAccuracy': 0.5}\n",
      "65 {'AvgEntryWiseED_L2': 0.10871053042821295, 'AvgEntryWiseED_L1': 0.049775714752093136, 'SparsityAccuracy': 0.5}\n",
      "66 {'AvgEntryWiseED_L2': 0.1085355805968139, 'AvgEntryWiseED_L1': 0.04976976450906339, 'SparsityAccuracy': 0.5}\n",
      "67 {'AvgEntryWiseED_L2': 0.10835903046370096, 'AvgEntryWiseED_L1': 0.04976372269478257, 'SparsityAccuracy': 0.5}\n",
      "68 {'AvgEntryWiseED_L2': 0.10818093409776805, 'AvgEntryWiseED_L1': 0.049757589275893556, 'SparsityAccuracy': 0.5}\n",
      "69 {'AvgEntryWiseED_L2': 0.1080013467889989, 'AvgEntryWiseED_L1': 0.04975136421851693, 'SparsityAccuracy': 0.5}\n",
      "70 {'AvgEntryWiseED_L2': 0.10782032506633628, 'AvgEntryWiseED_L1': 0.04974504748824995, 'SparsityAccuracy': 0.5}\n",
      "71 {'AvgEntryWiseED_L2': 0.10763792671569589, 'AvgEntryWiseED_L1': 0.04973863905016569, 'SparsityAccuracy': 0.5}\n",
      "72 {'AvgEntryWiseED_L2': 0.10745421079810932, 'AvgEntryWiseED_L1': 0.04973358744693543, 'SparsityAccuracy': 0.5}\n",
      "73 {'AvgEntryWiseED_L2': 0.10726923766797834, 'AvgEntryWiseED_L1': 0.04973111465726349, 'SparsityAccuracy': 0.5}\n",
      "74 {'AvgEntryWiseED_L2': 0.10708306899142209, 'AvgEntryWiseED_L1': 0.04972860763463089, 'SparsityAccuracy': 0.5}\n",
      "75 {'AvgEntryWiseED_L2': 0.10689576776469698, 'AvgEntryWiseED_L1': 0.04972606637385989, 'SparsityAccuracy': 0.5}\n",
      "76 {'AvgEntryWiseED_L2': 0.1067073983326682, 'AvgEntryWiseED_L1': 0.04972349086970098, 'SparsityAccuracy': 0.5}\n",
      "77 {'AvgEntryWiseED_L2': 0.10651802640730967, 'AvgEntryWiseED_L1': 0.04972088111683282, 'SparsityAccuracy': 0.5}\n",
      "78 {'AvgEntryWiseED_L2': 0.10632771908620826, 'AvgEntryWiseED_L1': 0.049718237109862166, 'SparsityAccuracy': 0.5}\n",
      "79 {'AvgEntryWiseED_L2': 0.10613654487104611, 'AvgEntryWiseED_L1': 0.049715558843323855, 'SparsityAccuracy': 0.5}\n",
      "80 {'AvgEntryWiseED_L2': 0.10594457368603354, 'AvgEntryWiseED_L1': 0.0497128463116807, 'SparsityAccuracy': 0.5}\n",
      "81 {'AvgEntryWiseED_L2': 0.10575187689626313, 'AvgEntryWiseED_L1': 0.049710099509323466, 'SparsityAccuracy': 0.5}\n",
      "82 {'AvgEntryWiseED_L2': 0.10555852732595387, 'AvgEntryWiseED_L1': 0.04970731843057082, 'SparsityAccuracy': 0.5}\n",
      "83 {'AvgEntryWiseED_L2': 0.10536459927655215, 'AvgEntryWiseED_L1': 0.04970450306966923, 'SparsityAccuracy': 0.5}\n",
      "84 {'AvgEntryWiseED_L2': 0.1051701685446549, 'AvgEntryWiseED_L1': 0.04970165342079294, 'SparsityAccuracy': 0.5}\n",
      "85 {'AvgEntryWiseED_L2': 0.10497531243971797, 'AvgEntryWiseED_L1': 0.04969876947804388, 'SparsityAccuracy': 0.5}\n",
      "86 {'AvgEntryWiseED_L2': 0.10478010980151051, 'AvgEntryWiseED_L1': 0.049695851235451644, 'SparsityAccuracy': 0.5}\n",
      "87 {'AvgEntryWiseED_L2': 0.10458464101727473, 'AvgEntryWiseED_L1': 0.049692898686973415, 'SparsityAccuracy': 0.5}\n",
      "88 {'AvgEntryWiseED_L2': 0.10438898803854739, 'AvgEntryWiseED_L1': 0.04968991182649388, 'SparsityAccuracy': 0.5}\n",
      "89 {'AvgEntryWiseED_L2': 0.10419323439759802, 'AvgEntryWiseED_L1': 0.049687206255620835, 'SparsityAccuracy': 0.5}\n",
      "90 {'AvgEntryWiseED_L2': 0.10399746522343595, 'AvgEntryWiseED_L1': 0.04968488689015409, 'SparsityAccuracy': 0.5}\n",
      "91 {'AvgEntryWiseED_L2': 0.10380176725733616, 'AvgEntryWiseED_L1': 0.04968254149989135, 'SparsityAccuracy': 0.5}\n",
      "92 {'AvgEntryWiseED_L2': 0.10360622886783172, 'AvgEntryWiseED_L1': 0.04968017008114675, 'SparsityAccuracy': 0.5}\n",
      "93 {'AvgEntryWiseED_L2': 0.10341094006511826, 'AvgEntryWiseED_L1': 0.04967777263019279, 'SparsityAccuracy': 0.5}\n",
      "94 {'AvgEntryWiseED_L2': 0.10321599251481298, 'AvgEntryWiseED_L1': 0.049675349143260356, 'SparsityAccuracy': 0.5}\n",
      "95 {'AvgEntryWiseED_L2': 0.10302147955100881, 'AvgEntryWiseED_L1': 0.04967289961653862, 'SparsityAccuracy': 0.5}\n",
      "96 {'AvgEntryWiseED_L2': 0.10282749618856205, 'AvgEntryWiseED_L1': 0.04967042404617511, 'SparsityAccuracy': 0.5}\n",
      "97 {'AvgEntryWiseED_L2': 0.10263413913454832, 'AvgEntryWiseED_L1': 0.049667922428275564, 'SparsityAccuracy': 0.5}\n",
      "98 {'AvgEntryWiseED_L2': 0.10244150679882064, 'AvgEntryWiseED_L1': 0.04966991047165298, 'SparsityAccuracy': 0.5}\n",
      "99 {'AvgEntryWiseED_L2': 0.10224969930359973, 'AvgEntryWiseED_L1': 0.0496741840187792, 'SparsityAccuracy': 0.5}\n"
     ]
    }
   ],
   "source": [
    "for N in range(1,100):\n",
    "    maps_ = {\n",
    "        edge : {\n",
    "            edge[0] : N/T * maps[edge][edge[0]],\n",
    "            edge[1] : N/T * maps[edge][edge[1]]\n",
    "        }\n",
    "        for edge in combinations(nodes, 2)\n",
    "    }\n",
    "    print(N,reconstructed_laplacian_metrics(len(nodes), edges, d, maps_, Y, L_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
