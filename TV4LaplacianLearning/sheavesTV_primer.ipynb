{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cellular sheaves on graphs \n",
    "## Learning sheaf laplacian through minimum total variation approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a toy-case topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a toy topology for our example\n",
    "\n",
    "nodes = [i for i in range(7)]\n",
    "edges = [\n",
    "    (0,1),\n",
    "    (0,2),\n",
    "    (0,6),\n",
    "    (1,3),\n",
    "    (1,5),\n",
    "    (2,3),\n",
    "    (2,4),\n",
    "    (3,4),\n",
    "    (4,6),\n",
    "    (5,6)\n",
    "]\n",
    "\n",
    "V = 7\n",
    "E = len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3                                           # Node and edges stalks dimension\n",
    "\n",
    "F = {\n",
    "    e:{\n",
    "        e[0]:np.random.randn(3,3),\n",
    "        e[1]:np.random.randn(3,3)\n",
    "        } \n",
    "        for e in edges\n",
    "    }                                           # Incidency linear maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph representation\n",
    "\n",
    "A = np.zeros((7,7))\n",
    "\n",
    "for edge in edges:\n",
    "    u = edge[0] \n",
    "    v = edge[1] \n",
    "\n",
    "    A[u,v] = 1\n",
    "    A[v,u] = 1\n",
    "\n",
    "D = np.diag(np.sum(A, axis = 0))\n",
    "L = D - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sheaf representation \n",
    "\n",
    "# Coboundary map\n",
    "\n",
    "B = np.zeros((d*E, d*V))\n",
    "\n",
    "for i in range(len(edges)):\n",
    "    edge = edges[i]\n",
    "\n",
    "    u = edge[0] \n",
    "    v = edge[1] \n",
    "\n",
    "    B_u = F[edge][u]\n",
    "    B_v = F[edge][v]\n",
    "\n",
    "    B[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "    B[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "# Sheaf Laplacian\n",
    "L_f = B.T @ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a smooth signals dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(from Hansen J., \"Learning sheaf Laplacians from smooth signals\")* \n",
    "\n",
    "In order to retrieve a dataset of smoothsignals, first of all we sample random gaussians vectors on the nodes of the graph. Then we smooth them according to their expansion in terms of the eigenvectors of the sheaf Laplacian $L_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's firstly define a dataset of random gaussian vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.random.randn(V*d,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the Fourier-domain embedded in the Laplacian spectrum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider a Tikhonov inspired procedure where we firstly project our dataset over the space spanned by the eigenvectors of the sheaf laplacian: namely $U$ the matrix collecting this eigenvectors we have \n",
    "\\begin{equation}\n",
    "    \\hat{x} = U^T x\n",
    "\\end{equation}\n",
    "\n",
    "So that defining $h(\\lambda) = \\frac{1}{1 + 10\\lambda}$ and $H = \\mathrm{diag}\\{h(\\lambda)\\}_{\\lambda}$, we now have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = H(\\Lambda) \\hat{x}\n",
    "\\end{equation}\n",
    "\n",
    "and finally our dataset is just reprojected back into the vertex domain:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = U H(\\Lambda) \\hat{x} = U H(\\Lambda) U^T x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda, U = np.linalg.eig(L_f)\n",
    "H = 1/(1 + 10*Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = U @ np.diag(H) @ U.T @ X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy the linear map learning strategy. If we consider each of the linear maps connecting the stalks over the nodes with the stalk of the inciding edge, we have that the minimum total variation on the Laplacian can be rewritten as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{\\mathcal{F}_{u \\triangleleft e}, \\mathcal{F}_{v \\triangleleft e}, e \\in E} \\frac{1}{2} \\sum_{e \\in E} \\sum_{i=1}^N ||\\mathcal{F}_{u \\triangleleft e}x_u^i - \\mathcal{F}_{v \\triangleleft e}x_v^i||_F^2\n",
    "\\end{equation}\n",
    "\n",
    "Clearly this problem can be decomposed into the contribution of each of the edges, requiring us to solve a subproblem for each of the possible edges: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{\\mathcal{F}_{u \\triangleleft e}, \\mathcal{F}_{v \\triangleleft e}} \\frac{1}{2} \\sum_{i=1}^N ||\\mathcal{F}_{u \\triangleleft e}x_u^i - \\mathcal{F}_{v \\triangleleft e}x_v^i||_F^2\n",
    "\\end{equation}\n",
    "\n",
    "This problem can be solved in a successive convex approximation fashion, being block-wise convex: the update equations are: \n",
    "\n",
    "\\begin{gather}\n",
    "    \\hat{{\\mathcal{F}}}_{u \\triangleleft e} = {\\mathcal{F}_{v \\triangleleft e}} (X_vX_u^T) (X_uX_u^T)^{-1} \\\\ \\nonumber\n",
    "    \\hat{{\\mathcal{F}}}_{v \\triangleleft e} = {\\mathcal{F}_{u \\triangleleft e}} (X_uX_v^T) (X_vX_v^T)^{-1}\n",
    "\\end{gather}\n",
    "\n",
    "In the end we can compute the total energy related to each edge: this means that if we sort out all the edges with respect to this measure we can rebuild the laplacian considering the given $t_0$ number of edges and the associated linear maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternated Linear Maps Learning\n",
    "\n",
    "def ALML(X_u, X_v, d, T = 500):\n",
    "    # Initialization \n",
    "\n",
    "    F_u = np.random.randn(d)\n",
    "    F_v = np.random.randn(d)\n",
    "    gamma = 0.99\n",
    "\n",
    "    # This matrices can be computed out of the learning loop \n",
    "\n",
    "    vu = X_v @ X_u.T\n",
    "    uv = X_u @ X_v.T\n",
    "    uu = np.linalg.inv(X_u @ X_u.T)\n",
    "    vv = np.linalg.inv(X_v @ X_v.T)\n",
    "\n",
    "    # Alternated learning through blockwise convex programs\n",
    "\n",
    "    for _ in range(T): \n",
    "        # Local step\n",
    "        F_u_hat = F_v @ vu @ uu\n",
    "        F_v_hat = F_u @ uv @ vv\n",
    "\n",
    "        # Convex smoothing\n",
    "        F_u = F_u + gamma*(F_u_hat - F_u)\n",
    "        F_v = F_v + gamma*(F_v_hat - F_v)\n",
    "\n",
    "        gamma *= 0.9\n",
    "        \n",
    "    return F_u, F_v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges = list(combinations(nodes, 2))\n",
    "maps = {\n",
    "    e:{\n",
    "        e[0] : np.zeros((3,3)), \n",
    "        e[1] : np.zeros((3,3))\n",
    "        } \n",
    "    for e in all_edges\n",
    "    }\n",
    "\n",
    "energies = {\n",
    "    e : 0\n",
    "    for e in all_edges\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 165.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for e in tqdm(all_edges):\n",
    "    u = e[0]\n",
    "    v = e[1]\n",
    "\n",
    "    X_u = Y[u*d:(u+1)*d,:]\n",
    "    X_v = Y[v*d:(v+1)*d,:]\n",
    "\n",
    "    F_u, F_v = ALML(X_u, X_v, d, T = 500)\n",
    "\n",
    "    maps[e][u] = F_u\n",
    "    maps[e][v] = F_v\n",
    "\n",
    "    L = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        x_u = X_u[:,i]\n",
    "        x_v = X_v[:,i]\n",
    "        L += np.linalg.norm(F_u @ x_u - F_v @ x_v)\n",
    "        \n",
    "    energies[e] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = sorted(energies.items(), key=lambda x:x[1])[:E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_hat = np.zeros((d*E, d*V))\n",
    "\n",
    "for i in range(10):\n",
    "    edge = retrieved[i][0]\n",
    "\n",
    "    u = edge[0] \n",
    "    v = edge[1] \n",
    "\n",
    "    B_u = maps[edge][u]\n",
    "    B_v = maps[edge][v]\n",
    "\n",
    "    B_hat[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "    B_hat[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "L_f_hat = B_hat.T @ B_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12678923441534412"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The metric chosen by Hansen for the evaluation was the average entry-wise euclidean distance\n",
    "\n",
    "np.sqrt(np.sum((L_f - L_f_hat)**2)) / L_f.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(V = 100, d = 3):\n",
    "    nodes = [n for n in range(V)]\n",
    "    edges = []\n",
    "\n",
    "    for u in range(V):\n",
    "        for v in range(u, V):\n",
    "            p = np.random.uniform(0,1,1)\n",
    "            if p > 0.5:\n",
    "                edges.append((u,v))\n",
    "\n",
    "    E = len(edges)\n",
    "    F = {\n",
    "        e:{\n",
    "            e[0]:np.random.randn(d,d),\n",
    "            e[1]:np.random.randn(d,d)\n",
    "            } \n",
    "            for e in edges\n",
    "        }                                           # Incidency linear maps\n",
    "\n",
    "    #_______________________\n",
    "    # Sheaf representation \n",
    "\n",
    "    B = np.zeros((d*E, d*V))                        # Coboundary maps\n",
    "\n",
    "    for i in range(len(edges)):\n",
    "\n",
    "        # Main loop to populate the coboundary map\n",
    "\n",
    "        edge = edges[i]\n",
    "\n",
    "        u = edge[0] \n",
    "        v = edge[1] \n",
    "\n",
    "        B_u = F[edge][u]\n",
    "        B_v = F[edge][v]\n",
    "\n",
    "        B[i*d:(i+1)*d, u*d:(u+1)*d] = B_u           \n",
    "        B[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "    L_f = B.T @ B\n",
    "\n",
    "    #_______________________\n",
    "\n",
    "    N = 100\n",
    "    X = np.random.randn(V*d,N) \n",
    "\n",
    "    # Spectral representation of the sheaf laplacian\n",
    "    Lambda, U = np.linalg.eig(L_f)\n",
    "\n",
    "    # Functional for filtering remapping the eigenvals in [0,1]\n",
    "    H = 1/(1 + 10*Lambda) \n",
    "\n",
    "    \n",
    "    Y = (U @                                        # Project back into the nodes domain\n",
    "         np.diag(H) @                               # Filter out in a Tikhonov fashion\n",
    "         U.T @ X                                    # Project gaussian random vectors in the Fourier domain of the sheaf laplacian \n",
    "    )\n",
    "\n",
    "    all_edges = list(combinations(nodes, 2))\n",
    "    maps = {\n",
    "        e:{\n",
    "            e[0] : np.zeros((d,d)), \n",
    "            e[1] : np.zeros((d,d))\n",
    "            } \n",
    "        for e in all_edges\n",
    "        }\n",
    "\n",
    "    energies = {\n",
    "        e : 0\n",
    "        for e in all_edges\n",
    "        }\n",
    "\n",
    "    for e in tqdm(all_edges):\n",
    "        u = e[0]\n",
    "        v = e[1]\n",
    "\n",
    "        X_u = Y[u*d:(u+1)*d,:]\n",
    "        X_v = Y[v*d:(v+1)*d,:]\n",
    "\n",
    "        F_u, F_v = ALML(X_u, X_v, d, T = 500)\n",
    "\n",
    "        maps[e][u] = F_u\n",
    "        maps[e][v] = F_v\n",
    "\n",
    "        L = 0\n",
    "\n",
    "        for i in range(100):\n",
    "            x_u = X_u[:,i]\n",
    "            x_v = X_v[:,i]\n",
    "            L += np.linalg.norm(F_u @ x_u - F_v @ x_v)\n",
    "            \n",
    "        energies[e] = L\n",
    "\n",
    "    retrieved = sorted(energies.items(), key=lambda x:x[1])[:E]\n",
    "    B_hat = np.zeros((d*E, d*V))\n",
    "\n",
    "    for i in range(E):\n",
    "        edge = retrieved[i][0]\n",
    "\n",
    "        u = edge[0] \n",
    "        v = edge[1] \n",
    "\n",
    "        B_u = maps[edge][u]\n",
    "        B_v = maps[edge][v]\n",
    "\n",
    "        B_hat[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "        B_hat[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "    L_f_hat = B_hat.T @ B_hat\n",
    "\n",
    "    return {\n",
    "        \"AvgEntryWiseED\" : np.sqrt(np.sum((L_f - L_f_hat)**2)) / L_f.size,\n",
    "        \"SparsityAccuracy\" : len(set(list(map(lambda x: x[0], retrieved))).intersection(set(edges))) / E\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20\n",
    "sims_1_avgDist = [0 for _ in range(T)]\n",
    "sims_1_sparsAcc = [0 for _ in range(T)]\n",
    "\n",
    "sims_2_avgDist = [0 for _ in range(T)]\n",
    "sims_2_sparsAcc = [0 for _ in range(T)]\n",
    "\n",
    "for t in range(T):\n",
    "    simu_1 = simulation(V = 100, d = 1)\n",
    "\n",
    "    sims_1_avgDist[t] = simu_1[\"AvgEntryWiseED\"]\n",
    "    sims_1_sparsAcc[t] = simu_1[\"SparsityAccuracy\"]\n",
    "\n",
    "    simu_2 = simulation(V = 50, d = 2)\n",
    "\n",
    "    sims_2_avgDist[t] = simu_2[\"AvgEntryWiseED\"]\n",
    "    sims_2_sparsAcc[t] = simu_2[\"SparsityAccuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG_sims_1_avgDist = sum(sims_1_avgDist) / 20\n",
    "AVG_sims_1_sparsAcc = sum(sims_1_sparsAcc) / 20\n",
    "AVG_sims_2_avgDist = sum(sims_2_avgDist) / 20\n",
    "AVG_sims_2_sparsAcc = sum(sims_2_sparsAcc) / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average entry-wise reconstruction error, V = 100, d = 1: 0.051820449777789934\n",
      "Average entry-wise reconstruction error, V = 50, d = 2:  0.05307085129770831\n"
     ]
    }
   ],
   "source": [
    "print(\"Average entry-wise reconstruction error, V = 100, d = 1:\", AVG_sims_1_avgDist)\n",
    "print(\"Average entry-wise reconstruction error, V = 50, d = 2: \", AVG_sims_2_avgDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision in recovering underlying graph, V = 100, d = 1: 0.4889273765031982\n",
      "Average precision in recovering underlying graph, V = 50, d = 2:  0.4564744248600553\n"
     ]
    }
   ],
   "source": [
    "print(\"Average precision in recovering underlying graph, V = 100, d = 1:\", AVG_sims_1_sparsAcc)\n",
    "print(\"Average precision in recovering underlying graph, V = 50, d = 2: \", AVG_sims_2_sparsAcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
