{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cellular sheaves on graphs \n",
    "## Learning sheaf laplacian through minimum total variation approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a toy-case topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a toy topology for our example\n",
    "\n",
    "nodes = [i for i in range(7)]\n",
    "edges = [\n",
    "    (0,1),\n",
    "    (0,2),\n",
    "    (0,6),\n",
    "    (1,3),\n",
    "    (1,5),\n",
    "    (2,3),\n",
    "    (2,4),\n",
    "    (3,4),\n",
    "    (4,6),\n",
    "    (5,6)\n",
    "]\n",
    "\n",
    "V = 7\n",
    "E = len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3                                           # Node and edges stalks dimension\n",
    "\n",
    "F = {\n",
    "    e:{\n",
    "        e[0]:np.random.randn(3,3),\n",
    "        e[1]:np.random.randn(3,3)\n",
    "        } \n",
    "        for e in edges\n",
    "    }                                           # Incidency linear maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph representation\n",
    "\n",
    "A = np.zeros((7,7))\n",
    "\n",
    "for edge in edges:\n",
    "    u = edge[0] \n",
    "    v = edge[1] \n",
    "\n",
    "    A[u,v] = 1\n",
    "    A[v,u] = 1\n",
    "\n",
    "D = np.diag(np.sum(A, axis = 0))\n",
    "L = D - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sheaf representation \n",
    "\n",
    "# Coboundary map\n",
    "\n",
    "B = np.zeros((d*E, d*V))\n",
    "\n",
    "for i in range(len(edges)):\n",
    "    edge = edges[i]\n",
    "\n",
    "    u = edge[0] \n",
    "    v = edge[1] \n",
    "\n",
    "    B_u = F[edge][u]\n",
    "    B_v = F[edge][v]\n",
    "\n",
    "    B[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "    B[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "# Sheaf Laplacian\n",
    "L_f = B.T @ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a smooth signals dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(from Hansen J., \"Learning sheaf Laplacians from smooth signals\")* \n",
    "\n",
    "In order to retrieve a dataset of smoothsignals, first of all we sample random gaussians vectors on the nodes of the graph. Then we smooth them according to their expansion in terms of the eigenvectors of the sheaf Laplacian $L_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's firstly define a dataset of random gaussian vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.random.randn(V*d,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the Fourier-domain embedded in the Laplacian spectrum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider a Tikhonov inspired procedure where we firstly project our dataset over the space spanned by the eigenvectors of the sheaf laplacian: namely $U$ the matrix collecting this eigenvectors we have \n",
    "\\begin{equation}\n",
    "    \\hat{x} = U^T x\n",
    "\\end{equation}\n",
    "\n",
    "So that defining $h(\\lambda) = \\frac{1}{1 + 10\\lambda}$ and $H = \\mathrm{diag}\\{h(\\lambda)\\}_{\\lambda}$, we now have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = H(\\Lambda) \\hat{x}\n",
    "\\end{equation}\n",
    "\n",
    "and finally our dataset is just reprojected back into the vertex domain:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = U H(\\Lambda) \\hat{x} = U H(\\Lambda) U^T x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda, U = np.linalg.eig(L_f)\n",
    "H = 1/(1 + 10*Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = U @ np.diag(H) @ U.T @ X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the different overall total variation from the original random sampled dataset of signals and the filtered one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16119.899062147002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(X.T @ L_f @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.343353785822271"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(Y.T @ L_f @ Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy the linear map learning strategy. If we consider each of the linear maps connecting the stalks over the nodes with the stalk of the inciding edge, we have that the minimum total variation on the Laplacian can be rewritten as (considering also a penalizing term that ensures that we avoid the trivial all-zero solutions *[Vassilis Kalofolias. “How to Learn a Graph from Smooth Signals.” In:\n",
    "Journal of Machine Learning Research (JMLR) (2016)]*):\n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{\\mathcal{F}_{u \\triangleleft e}, \\mathcal{F}_{v \\triangleleft e}, e \\in E} \\frac{1}{2} \\sum_{e \\in E} \\sum_{i=1}^N ||\\mathcal{F}_{u \\triangleleft e}x_u^i - \\mathcal{F}_{v \\triangleleft e}x_v^i||_F^2 - \\lambda_u \\log(\\det(\\mathcal{F}_{u \\triangleleft e})) - \\lambda_v \\log(\\det(\\mathcal{F}_{v \\triangleleft e}))\n",
    "\\end{equation}\n",
    "\n",
    "Clearly this problem can be decomposed into the contribution of each of the edges, requiring us to solve a subproblem for each of the possible edges: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{\\mathcal{F}_{u \\triangleleft e}, \\mathcal{F}_{v \\triangleleft e}} \\frac{1}{2} \\sum_{i=1}^N ||\\mathcal{F}_{u \\triangleleft e}x_u^i - \\mathcal{F}_{v \\triangleleft e}x_v^i||_F^2 - \\lambda_u \\log(\\det(\\mathcal{F}_{u \\triangleleft e})) - \\lambda_v \\log(\\det(\\mathcal{F}_{v \\triangleleft e}))\n",
    "\\end{equation}\n",
    "\n",
    "This problem can be solved in a successive convex approximation fashion, being block-wise convex. The update equations are the following, leveraging the results of *H. H. Bauschke and P. L. Combettes: Convex Analysis and Monotone Operator Theory in Hilbert Spaces (2nd Edition). Springer, New York, 2017* in deriving the proximal mapping of the logdet of a matrix. In particular, given $X \\in \\mathbb{R}^{n \\times n}$ and its spectral decomposition $X = U^T \\mathrm{Diag}(s) U$, the proximal mapping of $-\\log\\det(X)$ is\n",
    " \n",
    "\\begin{equation}\n",
    "\\mathrm{prox}_{-\\gamma\\log\\det(X)} = U \\ \\mathrm{Diag}(z) \\ U^T, \\ z = \\frac{1}{2}(s + \\sqrt{s^2 + 4\\gamma})\n",
    "\\end{equation}\n",
    "\n",
    "So finally our successive convex approximation follows the following update rules for $t = 0, ..., T$: \n",
    "\n",
    "\\begin{gather}\n",
    "    \\hat{{\\mathcal{F}}}_{u \\triangleleft e} = \\mathrm{prox}_{-\\lambda_u\\log\\det()}[{\\mathcal{F}_{v \\triangleleft e}^t} (X_vX_u^T) (X_uX_u^T)^{-1}] \\\\ \\nonumber\n",
    "    \\hat{{\\mathcal{F}}}_{v \\triangleleft e} = \\mathrm{prox}_{-\\lambda_v\\log\\det()}[{\\mathcal{F}_{u \\triangleleft e}^t} (X_uX_v^T) (X_vX_v^T)^{-1}] \\\\ \\nonumber\n",
    "    {\\mathcal{F}}_{u \\triangleleft e}^{t+1} = {\\mathcal{F}}_{u \\triangleleft e}^t - \\eta^t [{\\hat{\\mathcal{F}}_{u \\triangleleft e}} - {\\mathcal{F}}_{u \\triangleleft e}^t] \\\\ \\nonumber\n",
    "    {\\mathcal{F}}_{v \\triangleleft e}^{t+1} = {\\mathcal{F}}_{v \\triangleleft e}^t - \\eta^t [{\\hat{\\mathcal{F}}_{v \\triangleleft e}} - {\\mathcal{F}}_{v \\triangleleft e}^t]\n",
    "\\end{gather}\n",
    "\n",
    "In the end we can compute the total energy related to each edge: this means that if we sort out all the edges with respect to this measure we can rebuild the laplacian considering the given $t_0$ number of edges and the associated linear maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def pseudo_solver_A(A, B, X, Y, L, M = 50):\n",
    "    A_ = A.T\n",
    "    for _ in range(M):\n",
    "        A_ -= np.linalg.inv(2*A_ @ X - B @ Y @ X.T) @ (A_.T @ X @ X.T @ A - B @ Y @ X.T @ A_ - L*np.eye(A_.shape[0]))\n",
    "    return A_.T\n",
    "\n",
    "def pseudo_solver_B(A, B, X, Y, L, M=50):\n",
    "    B_ = B.T\n",
    "    for _ in range(M):\n",
    "        B_ -= np.linalg.inv(2*B_ @ Y - A @ X @ Y.T) @ (B_.T @ Y @ Y.T @ B - A @ X @ Y.T @ B_ - L*np.eye(B_.shape[0]))\n",
    "    return B_.T\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternated Linear Maps Learning\n",
    "\n",
    "def ALML(X_u, X_v, d, lambda_v = 0.5, lambda_u = 0.5, T = 500):\n",
    "    # Initialization \n",
    "\n",
    "    F_u = np.random.randn(d,d)\n",
    "    F_v = np.random.randn(d,d)\n",
    "    gamma = 0.99\n",
    "\n",
    "    # This matrices can be computed out of the learning loop \n",
    "\n",
    "    vu = X_v @ X_u.T\n",
    "    uv = X_u @ X_v.T\n",
    "    uu = np.linalg.inv(X_u @ X_u.T)\n",
    "    vv = np.linalg.inv(X_v @ X_v.T)\n",
    "\n",
    "    # Alternated learning through blockwise convex programs\n",
    "\n",
    "    for _ in range(T): \n",
    "\n",
    "        # Local step\n",
    "\n",
    "        # Close form without regularization\n",
    "        F_u_hat = F_v @ vu @ uu \n",
    "        F_v_hat = F_u @ uv @ vv \n",
    "\n",
    "        # Proximal mapping\n",
    "        s_u, U_u = np.linalg.eig(F_u_hat)\n",
    "        z_u = 0.5*(s_u + np.sqrt(s_u**2 + 4*lambda_u))\n",
    "        F_u_hat = U_u @ np.diag(z_u) @ U_u.T\n",
    "        \n",
    "        s_v, U_v = np.linalg.eig(F_v_hat)\n",
    "        z_v = 0.5*(s_v + np.sqrt(s_v**2 + 4*lambda_v))\n",
    "        F_v_hat = U_v @ np.diag(z_v) @ U_v.T\n",
    "        \n",
    "        '''\n",
    "        # Pseudo solving\n",
    "        F_u_hat = pseudo_solver_A(F_u, F_v, X_u, X_v, lambda_u)\n",
    "        F_v_hat = pseudo_solver_A(F_v, F_v, X_u, X_v, lambda_u)\n",
    "        '''\n",
    "        # Convex smoothing\n",
    "        F_u = F_u + gamma*(F_u_hat - F_u)\n",
    "        F_v = F_v + gamma*(F_v_hat - F_v)\n",
    "\n",
    "        gamma *= 0.9\n",
    "        \n",
    "    return F_u, F_v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges = list(combinations(nodes, 2))\n",
    "maps = {\n",
    "    e:{\n",
    "        e[0] : np.zeros((3,3)), \n",
    "        e[1] : np.zeros((3,3))\n",
    "        } \n",
    "    for e in all_edges\n",
    "    }\n",
    "\n",
    "energies = {\n",
    "    e : 0\n",
    "    for e in all_edges\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 25.49it/s]\n"
     ]
    }
   ],
   "source": [
    "for e in tqdm(all_edges):\n",
    "    u = e[0]\n",
    "    v = e[1]\n",
    "\n",
    "    X_u = Y[u*d:(u+1)*d,:]\n",
    "    X_v = Y[v*d:(v+1)*d,:]\n",
    "\n",
    "    F_u, F_v = ALML(X_u, X_v, d, T = 500)\n",
    "\n",
    "    maps[e][u] = F_u\n",
    "    maps[e][v] = F_v\n",
    "\n",
    "    L = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        x_u = X_u[:,i]\n",
    "        x_v = X_v[:,i]\n",
    "        L += np.linalg.norm(F_u @ x_u - F_v @ x_v)\n",
    "        \n",
    "    energies[e] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = sorted(energies.items(), key=lambda x:x[1])[:E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_hat = np.zeros((d*E, d*V))\n",
    "\n",
    "for i in range(10):\n",
    "    edge = retrieved[i][0]\n",
    "\n",
    "    u = edge[0] \n",
    "    v = edge[1] \n",
    "\n",
    "    B_u = maps[edge][u]\n",
    "    B_v = maps[edge][v]\n",
    "\n",
    "    B_hat[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "    B_hat[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "L_f_hat = B_hat.T @ B_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10201050867588193"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The metric chosen by Hansen for the evaluation was the average entry-wise euclidean distance\n",
    "\n",
    "np.sqrt(np.sum((L_f - L_f_hat)**2)) / L_f.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(list(map(lambda x: x[0], retrieved))).intersection(set(edges))) / E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(V = 100, d = 3):\n",
    "    nodes = [n for n in range(V)]\n",
    "    edges = []\n",
    "\n",
    "    for u in range(V):\n",
    "        for v in range(u, V):\n",
    "            p = np.random.uniform(0,1,1)\n",
    "            if p > 0.5:\n",
    "                edges.append((u,v))\n",
    "\n",
    "    E = len(edges)\n",
    "    F = {\n",
    "        e:{\n",
    "            e[0]:np.random.randn(d,d),\n",
    "            e[1]:np.random.randn(d,d)\n",
    "            } \n",
    "            for e in edges\n",
    "        }                                           # Incidency linear maps\n",
    "\n",
    "    #_______________________\n",
    "    # Sheaf representation \n",
    "\n",
    "    B = np.zeros((d*E, d*V))                        # Coboundary maps\n",
    "\n",
    "    for i in range(len(edges)):\n",
    "\n",
    "        # Main loop to populate the coboundary map\n",
    "\n",
    "        edge = edges[i]\n",
    "\n",
    "        u = edge[0] \n",
    "        v = edge[1] \n",
    "\n",
    "        B_u = F[edge][u]\n",
    "        B_v = F[edge][v]\n",
    "\n",
    "        B[i*d:(i+1)*d, u*d:(u+1)*d] = B_u           \n",
    "        B[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "    L_f = B.T @ B\n",
    "\n",
    "    #_______________________\n",
    "\n",
    "    N = 100\n",
    "    X = np.random.randn(V*d,N) \n",
    "\n",
    "    # Spectral representation of the sheaf laplacian\n",
    "    Lambda, U = np.linalg.eig(L_f)\n",
    "\n",
    "    # Functional for filtering remapping the eigenvals in [0,1]\n",
    "    H = 1/(1 + 10*Lambda) \n",
    "\n",
    "    \n",
    "    Y = (U @                                        # Project back into the nodes domain\n",
    "         np.diag(H) @                               # Filter out in a Tikhonov fashion\n",
    "         U.T @ X                                    # Project gaussian random vectors in the Fourier domain of the sheaf laplacian \n",
    "    )\n",
    "\n",
    "    all_edges = list(combinations(nodes, 2))\n",
    "    maps = {\n",
    "        e:{\n",
    "            e[0] : np.zeros((d,d)), \n",
    "            e[1] : np.zeros((d,d))\n",
    "            } \n",
    "        for e in all_edges\n",
    "        }\n",
    "\n",
    "    energies = {\n",
    "        e : 0\n",
    "        for e in all_edges\n",
    "        }\n",
    "\n",
    "    for e in tqdm(all_edges):\n",
    "        u = e[0]\n",
    "        v = e[1]\n",
    "\n",
    "        X_u = Y[u*d:(u+1)*d,:]\n",
    "        X_v = Y[v*d:(v+1)*d,:]\n",
    "\n",
    "        F_u, F_v = ALML(X_u, X_v, d, T = 500)\n",
    "\n",
    "        maps[e][u] = F_u\n",
    "        maps[e][v] = F_v\n",
    "\n",
    "        L = 0\n",
    "\n",
    "        for i in range(100):\n",
    "            x_u = X_u[:,i]\n",
    "            x_v = X_v[:,i]\n",
    "            L += np.linalg.norm(F_u @ x_u - F_v @ x_v)\n",
    "            \n",
    "        energies[e] = L\n",
    "\n",
    "    retrieved = sorted(energies.items(), key=lambda x:x[1])[:E]\n",
    "    B_hat = np.zeros((d*E, d*V))\n",
    "\n",
    "    for i in range(E):\n",
    "        edge = retrieved[i][0]\n",
    "\n",
    "        u = edge[0] \n",
    "        v = edge[1] \n",
    "\n",
    "        B_u = maps[edge][u]\n",
    "        B_v = maps[edge][v]\n",
    "\n",
    "        B_hat[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
    "        B_hat[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "    L_f_hat = B_hat.T @ B_hat\n",
    "\n",
    "    return {\n",
    "        \"AvgEntryWiseED\" : np.sqrt(np.sum((L_f - L_f_hat)**2)) / L_f.size,\n",
    "        \"SparsityAccuracy\" : len(set(list(map(lambda x: x[0], retrieved))).intersection(set(edges))) / E\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4950/4950 [02:39<00:00, 31.04it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.18it/s]\n",
      "C:\\Users\\Leonardo\\AppData\\Local\\Temp\\ipykernel_18008\\2137430888.py:106: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  B_hat[i*d:(i+1)*d, u*d:(u+1)*d] = B_u\n",
      "C:\\Users\\Leonardo\\AppData\\Local\\Temp\\ipykernel_18008\\2137430888.py:107: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  B_hat[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
      "100%|██████████| 4950/4950 [02:39<00:00, 31.11it/s]\n",
      "100%|██████████| 1225/1225 [00:42<00:00, 28.98it/s]\n",
      "100%|██████████| 4950/4950 [02:38<00:00, 31.15it/s]\n",
      "100%|██████████| 1225/1225 [00:42<00:00, 29.15it/s]\n",
      "100%|██████████| 4950/4950 [02:39<00:00, 31.10it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.22it/s]\n",
      "100%|██████████| 4950/4950 [02:39<00:00, 31.13it/s]\n",
      "100%|██████████| 1225/1225 [00:42<00:00, 29.05it/s]\n",
      "100%|██████████| 4950/4950 [02:38<00:00, 31.17it/s]\n",
      "100%|██████████| 1225/1225 [00:42<00:00, 29.11it/s]\n",
      "100%|██████████| 4950/4950 [02:39<00:00, 31.11it/s]\n",
      "100%|██████████| 1225/1225 [00:42<00:00, 29.10it/s]\n",
      "100%|██████████| 4950/4950 [02:41<00:00, 30.72it/s]\n",
      "100%|██████████| 1225/1225 [00:42<00:00, 29.08it/s]\n",
      "100%|██████████| 4950/4950 [02:39<00:00, 31.05it/s]\n",
      "100%|██████████| 1225/1225 [00:42<00:00, 29.12it/s]\n",
      "100%|██████████| 4950/4950 [02:38<00:00, 31.18it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.39it/s]\n",
      "100%|██████████| 4950/4950 [02:37<00:00, 31.43it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.34it/s]\n",
      "100%|██████████| 4950/4950 [02:39<00:00, 31.04it/s]\n",
      "100%|██████████| 1225/1225 [00:42<00:00, 28.98it/s]\n",
      "100%|██████████| 4950/4950 [02:38<00:00, 31.14it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.36it/s]\n",
      "100%|██████████| 4950/4950 [02:37<00:00, 31.45it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.25it/s]\n",
      "100%|██████████| 4950/4950 [02:38<00:00, 31.22it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.33it/s]\n",
      "100%|██████████| 4950/4950 [02:39<00:00, 30.98it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.34it/s]\n",
      "100%|██████████| 4950/4950 [02:38<00:00, 31.31it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.42it/s]\n",
      "100%|██████████| 4950/4950 [02:37<00:00, 31.46it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.43it/s]\n",
      "100%|██████████| 4950/4950 [02:37<00:00, 31.43it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.40it/s]\n",
      "100%|██████████| 4950/4950 [02:37<00:00, 31.33it/s]\n",
      "100%|██████████| 1225/1225 [00:41<00:00, 29.38it/s]\n"
     ]
    }
   ],
   "source": [
    "T = 20\n",
    "sims_1_avgDist = [0 for _ in range(T)]\n",
    "sims_1_sparsAcc = [0 for _ in range(T)]\n",
    "\n",
    "sims_2_avgDist = [0 for _ in range(T)]\n",
    "sims_2_sparsAcc = [0 for _ in range(T)]\n",
    "\n",
    "for t in range(T):\n",
    "    simu_1 = simulation(V = 100, d = 1)\n",
    "\n",
    "    sims_1_avgDist[t] = simu_1[\"AvgEntryWiseED\"]\n",
    "    sims_1_sparsAcc[t] = simu_1[\"SparsityAccuracy\"]\n",
    "\n",
    "    simu_2 = simulation(V = 50, d = 2)\n",
    "\n",
    "    sims_2_avgDist[t] = simu_2[\"AvgEntryWiseED\"]\n",
    "    sims_2_sparsAcc[t] = simu_2[\"SparsityAccuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG_sims_1_avgDist = sum(sims_1_avgDist) / 20\n",
    "AVG_sims_1_sparsAcc = sum(sims_1_sparsAcc) / 20\n",
    "AVG_sims_2_avgDist = sum(sims_2_avgDist) / 20\n",
    "AVG_sims_2_sparsAcc = sum(sims_2_sparsAcc) / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average entry-wise reconstruction error, V = 100, d = 1: 0.026654062783924775\n",
      "Average entry-wise reconstruction error, V = 50, d = 2:  0.03701907657856198\n"
     ]
    }
   ],
   "source": [
    "print(\"Average entry-wise reconstruction error, V = 100, d = 1:\", AVG_sims_1_avgDist)\n",
    "print(\"Average entry-wise reconstruction error, V = 50, d = 2: \", AVG_sims_2_avgDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision in recovering underlying graph, V = 100, d = 1: 0.5219278697765474\n",
      "Average precision in recovering underlying graph, V = 50, d = 2:  0.5394094368121383\n"
     ]
    }
   ],
   "source": [
    "print(\"Average precision in recovering underlying graph, V = 100, d = 1:\", AVG_sims_1_sparsAcc)\n",
    "print(\"Average precision in recovering underlying graph, V = 50, d = 2: \", AVG_sims_2_sparsAcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
